#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
fact_checker.py
--------------
ç¦»çº¿ MiniLM + softâ€‘prompt + BM25/TFâ€‘IDF æ£€ç´¢ + LR åˆ†ç±»å™¨
å¦å« Topâ€‘K æ£€ç´¢ç»“æœå¯¼å‡ºä¸è¯„ä¼° (Recall@K / Precision@K)
"""

import os, json, argparse, pickle, pathlib, sys, warnings, tarfile
import numpy as np, joblib, tqdm, torch, nltk, re
from sentence_transformers import SentenceTransformer
from sklearn.linear_model import LogisticRegression
from sklearn.feature_extraction.text import TfidfVectorizer
from rank_bm25 import BM25Okapi

# ============ å¸¸é‡åŒº ============
DATA_DIR   = "COMP90042_2025/data"
EVI_PATH   = f"{DATA_DIR}/evidence.json"

LOCAL_MODEL_PATH = "./local_model"        # æœ¬åœ° MiniLM æ–‡ä»¶å¤¹
MODEL_NAME       = "all-MiniLM-L6-v2"     # ä»…ä½œç¼“å­˜å›é€€å
BATCH_SIZE       = 64
MAX_EVI          = 5      # ä¼ ç»™åˆ†ç±»å™¨çš„ evidence æ•°
KBM25            = 200    # åˆç­›å€™é€‰æ•°
TOPK_DEFAULT     = 100    # dump / eval é»˜è®¤ K
DEVICE           = torch.device("cuda" if torch.cuda.is_available() else "cpu")

LABEL_MAP  = {"SUPPORTS":0, "REFUTES":1, "NOT_ENOUGH_INFO":2, "DISPUTED":3}
ID2LABEL   = {v:k for k,v in LABEL_MAP.items()}
# ================================

# --------- ç¯å¢ƒå‡†å¤‡ï¼šNLTK èµ„æº ---------
for res in ("punkt","stopwords","punkt_tab"):
    try:
        path = (f"tokenizers/{res}" if res.startswith("punkt") else f"corpora/{res}")
        nltk.data.find(path)
    except LookupError:
        nltk.download(res, quiet=True)

# --------- å¥å‘é‡ç¼–ç å™¨åŠ è½½ ----------
def build_sentence_encoder():
    print(f"ğŸ” Loading Sentenceâ€‘Transformer on {DEVICE} â€¦")
    try:
        if os.path.isdir(LOCAL_MODEL_PATH):
            print(f" â†’ from local dir: {LOCAL_MODEL_PATH}")
            return SentenceTransformer(LOCAL_MODEL_PATH, device=DEVICE, trust_remote_code=True)
        print(" â†’ local dir missing, trying HF cache â€¦")
        return SentenceTransformer(MODEL_NAME, device=DEVICE, trust_remote_code=True)
    except Exception as e:
        sys.exit(f"âŒ Failed to load encoder: {e}\nè¯·æ£€æŸ¥ {LOCAL_MODEL_PATH} æ˜¯å¦å«æ¨¡å‹æ–‡ä»¶")

# --------- ç®€æ˜“ tokenizer (é¿å… punkt ä¾èµ–è¿‡é‡ï¼Œå¯æ›¿æ¢) ---------
_word_pat = re.compile(r"\b\w+\b")
def quick_tokenize(text:str):
    return _word_pat.findall(text.lower())

# ------------------ Retriever ------------------
class Retriever:
    def __init__(self, mode="bm25", rebuild=False, soft_prompt=""):
        assert mode in ("bm25","tfidf")
        self.mode        = mode
        self.soft_prompt = soft_prompt

        self.evidence = json.load(open(EVI_PATH))
        self.ids, self.texts = zip(*self.evidence.items())
        self.idx_pkl = f"retr_{mode}.pkl"

        if rebuild or not pathlib.Path(self.idx_pkl).exists():
            self._build_index()
        else:
            self._load_index()

        # è¯­ä¹‰é‡æ’
        self.encoder = build_sentence_encoder()
        if rebuild or not pathlib.Path("emb.npy").exists():
            self._encode_evidence()
        else:
            self._load_embeddings()

    # ---------- ç´¢å¼• ----------
    def _build_index(self):
        print(f"[Retriever] build {self.mode} index")
        if self.mode == "bm25":
            tokenised = [quick_tokenize(t) for t in self.texts]
            self.bm25 = BM25Okapi(tokenised)
            joblib.dump({"bm25": self.bm25}, self.idx_pkl)
        else:
            self.vectorizer = TfidfVectorizer(stop_words="english",
                                              ngram_range=(1,2),
                                              max_df=0.9, min_df=2)
            self.evi_mat = self.vectorizer.fit_transform(self.texts)
            joblib.dump({"vec": self.vectorizer, "mat": self.evi_mat}, self.idx_pkl)

    def _load_index(self):
        obj = joblib.load(self.idx_pkl)
        if "bm25" in obj:
            self.bm25 = obj["bm25"]
        else:
            self.vectorizer, self.evi_mat = obj["vec"], obj["mat"]

    # ---------- evidence å‘é‡ ----------
    def _encode_evidence(self):
        print("[Retriever] encode evidence embeddings â€¦")
        embs = self.encoder.encode(self.texts, batch_size=BATCH_SIZE,
                                   normalize_embeddings=True,
                                   show_progress_bar=True)
        np.save("emb.npy", embs)
        pickle.dump(self.ids, open("ids.pkl","wb"))
        self.emb_dict = dict(zip(self.ids, embs))

    def _load_embeddings(self):
        self.ids      = pickle.load(open("ids.pkl","rb"))
        self.emb_dict = dict(zip(self.ids, np.load("emb.npy")))

    # ---------- æŸ¥è¯¢ ----------
    def retrieve(self, claim:str, k:int=MAX_EVI, return_scores=False):
        claim_aug = f"{self.soft_prompt} {claim}".strip()

        # ç¬¬ä¸€é˜¶æ®µ
        if self.mode=="bm25":
            scores = self.bm25.get_scores(quick_tokenize(claim_aug))
        else:
            qv = self.vectorizer.transform([claim_aug])
            scores = (qv @ self.evi_mat.T).toarray()[0]
        top_idx = np.argsort(scores)[::-1][:KBM25]

        # è¯­ä¹‰é‡æ’
        qvec = self.encoder.encode(claim_aug, normalize_embeddings=True)
        reranked = [(self.ids[i], float(qvec @ self.emb_dict[self.ids[i]]))
                    for i in top_idx]
        reranked.sort(key=lambda x:x[1], reverse=True)
        top = reranked[:k]

        if return_scores:
            return top                      # list[(eid,score)]
        return [eid for eid,_ in top], [self.evidence[eid] for eid,_ in top]

# ------------------ åˆ†ç±»å™¨è®­ç»ƒ ------------------
def train_classifier(retr_mode, soft_prompt):
    retr    = Retriever(retr_mode, soft_prompt=soft_prompt)
    encoder = build_sentence_encoder()

    def build(split):
        claims = json.load(open(f"{DATA_DIR}/{split}-claims.json"))
        X, y = [], []
        for c in tqdm.tqdm(claims.values(), desc=f"encode {split}"):
            ev = [retr.evidence[eid] for eid in c["evidences"][:MAX_EVI]]
            text = f"{soft_prompt} {c['claim_text']} </s> " + " ".join(ev)
            X.append(text.strip())
            y.append(LABEL_MAP[c["claim_label"]])
        Xemb = encoder.encode(X, batch_size=BATCH_SIZE, normalize_embeddings=False,
                              show_progress_bar=True)
        return Xemb, np.array(y)

    X, y = build("train")
    clf  = LogisticRegression(max_iter=1000, multi_class="ovr")
    clf.fit(X, y)
    joblib.dump({"clf":clf, "soft_prompt":soft_prompt, "retr_mode":retr_mode},
                "logreg_mini.joblib")
    print(f"[Classifier] savedâ€ƒTrain acc={clf.score(X,y):.4f}")

# ------------------ é¢„æµ‹ ------------------
def run_split(claim_json, out_json, retr_mode, soft_prompt, limit=None):
    if not pathlib.Path("logreg_mini.joblib").exists():
        sys.exit("âŒ è¯·å…ˆè®­ç»ƒåˆ†ç±»å™¨ (--train)")
    pkg = joblib.load("logreg_mini.joblib")
    if soft_prompt!=pkg["soft_prompt"]:
        warnings.warn("å½“å‰ softâ€‘prompt ä¸è®­ç»ƒä¸ä¸€è‡´ï¼")

    retr = Retriever(retr_mode, soft_prompt=soft_prompt)
    enc  = build_sentence_encoder()
    clf  = pkg["clf"]

    claims = json.load(open(claim_json))
    if limit: claims = dict(list(claims.items())[:limit])

    results = {}
    for cid,c in tqdm.tqdm(claims.items(), desc="predict"):
        eids, etexts = retr.retrieve(c["claim_text"])
        if etexts:
            vec = enc.encode(f"{soft_prompt} {c['claim_text']} </s> "
                             + " ".join(etexts),
                             normalize_embeddings=False)
            label = ID2LABEL[int(clf.predict([vec])[0])]
        else:
            label = "NOT_ENOUGH_INFO"
        results[cid] = {"claim_label":label, "evidences":eids}
    json.dump(results, open(out_json,"w"), indent=2)
    print(f"[Output] {out_json} saved")

# ------------------ Dev è¯„ä¼° ------------------
def evaluate(pred_json, gold_json):
    p, g = json.load(open(pred_json)), json.load(open(gold_json))
    f_list, acc_list = [], []
    for cid,true in g.items():
        if cid not in p: continue
        pred = p[cid]
        acc_list.append(pred["claim_label"]==true["claim_label"])
        tp = len(set(pred["evidences"]) & set(true["evidences"]))
        prec = tp/len(pred["evidences"]) if pred["evidences"] else 0
        rec  = tp/len(true["evidences"])
        f    = 0 if prec+rec==0 else 2*prec*rec/(prec+rec)
        f_list.append(f)
    F, A = np.mean(f_list), np.mean(acc_list)
    H    = 0 if F+A==0 else 2*F*A/(F+A)
    print(f"\n[Dev] Retrievalâ€‘F={F:.4f}  Acc={A:.4f}  Harmonic={H:.4f}\n")

# ------------------ Topâ€‘K dump & è¯„ä¼° ------------------
def dump_topk(split_json, out_json, retr_mode, soft_prompt, topk):
    retr   = Retriever(retr_mode, soft_prompt=soft_prompt)
    claims = json.load(open(split_json))
    out = {}
    for cid,c in tqdm.tqdm(claims.items(), desc=f"Top{topk}"):
        eids_scores = retr.retrieve(c["claim_text"], k=topk, return_scores=True)
        out[cid] = {"evidences":[eid for eid,_ in eids_scores]}
    json.dump(out, open(out_json,"w"), indent=2)
    print(f"[TopK] {out_json} saved")

def eval_topk(train_claims_path, topk_json):
    train_claims = json.load(open(train_claims_path))
    topk_pred    = json.load(open(topk_json))

    recalls, precisions = [], []
    for cid,c in train_claims.items():
        gt  = set(c.get("evidences", []))
        ret = set(topk_pred.get(cid, {}).get("evidences", []))
        tp  = len(gt & ret)
        recalls.append(tp/len(gt) if gt else 0)
        precisions.append(tp/len(ret) if ret else 0)

    R = sum(recalls) / len(recalls)
    P = sum(precisions) / len(precisions)
    k_val = len(next(iter(topk_pred.values()))["evidences"])  
    print(f"\n=== Retrieval Top{k_val} ===")
    print(f"Average Recall   : {R:.3f}")
    print(f"Average Precision: {P:.3f}\n")


# ------------------ CLI ------------------
def cli():
    ap = argparse.ArgumentParser()
    ap.add_argument("--rebuild-index", action="store_true")
    ap.add_argument("--train",         action="store_true")
    ap.add_argument("--dev-eval",      action="store_true")
    ap.add_argument("--predict-test",  action="store_true")
    ap.add_argument("--dump-topk",     action="store_true",
                    help="å¯¼å‡º <split>-claims-topK.json")
    ap.add_argument("--eval-topk",     action="store_true",
                    help="è®¡ç®— TopK æ£€ç´¢ Recall/Precision")
    ap.add_argument("--topk", type=int, default=TOPK_DEFAULT,
                    help=f"K for TopK retrieval (default {TOPK_DEFAULT})")
    ap.add_argument("--split", choices=["train","dev","test"], default="dev",
                    help="split é€‰æ‹© (dump/eval ç”¨)")
    ap.add_argument("--limit", type=int)
    ap.add_argument("--retrieval", choices=["bm25","tfidf"], default="bm25")
    ap.add_argument("--soft-prompt", default="")
    return ap.parse_args()

# ------------------ ä¸»å…¥å£ ------------------
if __name__=="__main__":
    # ç¦»çº¿æ¨¡å¼ç¯å¢ƒå˜é‡ï¼ˆè‹¥è¦è”ç½‘åŠ è½½å¯æ³¨é‡Šæ‰ï¼‰
    os.environ["HF_HUB_OFFLINE"]="1"; os.environ["TRANSFORMERS_OFFLINE"]="1"

    args = cli()

    # å¦‚éœ€é‡å»ºç´¢å¼• / emb
    Retriever(mode=args.retrieval, rebuild=args.rebuild_index,
              soft_prompt=args.soft_prompt)

    # ---------- Topâ€‘K åŠŸèƒ½ ----------
    if args.dump_topk:
        split_file = (f"{DATA_DIR}/{args.split}-claims.json" if args.split!="test"
                      else f"{DATA_DIR}/test-claims-unlabelled.json")
        out_file = f"{args.split}-claims-top{args.topk}.json"
        dump_topk(split_file, out_file,
                  args.retrieval, args.soft_prompt, args.topk)
    if args.eval_topk:
        if args.split=="test":
            sys.exit("âŒ test split æ— æ ‡ç­¾ï¼Œæ— æ³•è¯„ä¼°")
        top_file = f"{args.split}-claims-top{args.topk}.json"
        if not pathlib.Path(top_file).exists():
            sys.exit(f"âŒ {top_file} ä¸å­˜åœ¨ï¼Œè¯·å…ˆ --dump-topk")
        eval_topk(f"{DATA_DIR}/{args.split}-claims.json", top_file)

    # ---------- åŸæœ‰æµæ°´çº¿ ----------
    if args.train:
        train_classifier(args.retrieval, soft_prompt=args.soft_prompt)
    if args.dev_eval:
        run_split(f"{DATA_DIR}/dev-claims.json", "dev-preds.json",
                  args.retrieval, args.soft_prompt, args.limit)
        evaluate("dev-preds.json", f"{DATA_DIR}/dev-claims.json")
    if args.predict_test:
        run_split(f"{DATA_DIR}/test-claims-unlabelled.json",
                  "test-claims-predictions.json",
                  args.retrieval, args.soft_prompt, args.limit)
        print("âœ… test-claims-predictions.json ready")

# ------------------ QuickÂ UsageÂ Cheatâ€‘Sheet ------------------
# 1) å»ºç´¢å¼• + è®­ç»ƒ + dev è¯„ä¼° + ç”Ÿæˆ test ç»“æœ
# python fact_checker.py --rebuild-index --train --dev-eval --predict-test \
#                        --soft-prompt "[CLS] Factâ€‘check the following climate statement:"
#
# 2) Dump dev split Topâ€‘100 æ£€ç´¢ç»“æœ
# python fact_checker.py --dump-topk --topk 100 --split dev
#
# 3) è¯„ä¼° Recall@100 / Precision@100
# python fact_checker.py --eval-topk --topk 100 --split dev
#
# 4) åªé¢„æµ‹ testï¼ˆå¤ç”¨å·²è®­ç»ƒåˆ†ç±»å™¨ï¼‰
# python fact_checker.py --predict-test --soft-prompt "[CLS] Factâ€‘check the following climate statement:"
