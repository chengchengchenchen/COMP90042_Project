#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
fact_checker_tfidf.py
---------------------
COMP90042 2025 â€“ çº¯ TF-IDF æ£€ç´¢ + MiniLM è¯­ä¹‰é‡æ’ + LR åˆ†ç±»
åŠŸèƒ½ï¼š
  â€¢ --rebuild-index   æ„å»º TF-IDF ç´¢å¼• / evidence embedding
  â€¢ --train           è®­ç»ƒåˆ†ç±»å™¨
  â€¢ --dev-eval        åœ¨ dev split ä¸Šé¢„æµ‹å¹¶è¯„ä¼°
  â€¢ --predict-test    ç”Ÿæˆ test-claims-predictions.json
  â€¢ --dump-topk       å¯¼å‡º Top-K evidenceï¼ˆtrain/dev/testï¼‰
  â€¢ --eval-topk       è®¡ç®—å¹³å‡ Recall@K / Precision@Kï¼ˆä»… train/devï¼‰

ç¤ºä¾‹ï¼š
  # æ„å»ºç´¢å¼• + è®­ç»ƒ + Dev è¯„ä¼°
  python fact_checker_tfidf.py --rebuild-index --train --dev-eval
  
  # ç”Ÿæˆ Test ç»“æœ
  python fact_checker_tfidf.py --predict-test
  
  # Top-100 æ£€ç´¢åˆ†æ
  python fact_checker_tfidf.py --dump-topk --topk 100 --split dev
  python fact_checker_tfidf.py --eval-topk --topk 100 --split dev
"""

# ========= ä¾èµ– =========
import os, json, argparse, pickle, pathlib, sys, warnings, re
import numpy as np, joblib, tqdm, torch, nltk
from sentence_transformers import SentenceTransformer
from sklearn.linear_model import LogisticRegression
from sklearn.feature_extraction.text import TfidfVectorizer
# ========================

# ---------- å¸¸é‡ ----------
DATA_DIR   = "COMP90042_2025/data"
EVI_PATH   = f"{DATA_DIR}/evidence.json"

LOCAL_MODEL_PATH = "./local_model"             # æœ¬åœ° MiniLMï¼ˆæ¨èç¦»çº¿æ”¾ç½®ï¼‰
MODEL_NAME       = "all-MiniLM-L6-v2"          # HF åç§°ï¼ˆç¦»çº¿å¤±è´¥æ—¶ç”¨ï¼‰
BATCH_SIZE       = 64
MAX_EVI_RET      = 5        # ä¼ å…¥åˆ†ç±»å™¨çš„ evidence æ•°
TOPK_CANDIDATE   = 200      # TF-IDF åˆç­›å€™é€‰æ•°
DEVICE           = torch.device("cuda" if torch.cuda.is_available() else "cpu")

LABEL_MAP = {"SUPPORTS":0, "REFUTES":1, "NOT_ENOUGH_INFO":2, "DISPUTED":3}
ID2LABEL  = {v:k for k,v in LABEL_MAP.items()}
# ==========================

# ---------- NLTK èµ„æº ----------
for res in ("punkt","stopwords","punkt_tab"):
    try:
        p = f"tokenizers/{res}" if res.startswith("punkt") else f"corpora/{res}"
        nltk.data.find(p)
    except LookupError:
        nltk.download(res, quiet=True)

# ---------- ç®€æ˜“ tokenizer ----------
_word_pat = re.compile(r"\b\w+\b")
def quick_tokenize(txt:str):
    return _word_pat.findall(txt.lower())

# ---------- Sentence-BERT ----------
def build_encoder():
    print(f"ğŸ” Loading MiniLM on {DEVICE} â€¦")
    try:
        if os.path.isdir(LOCAL_MODEL_PATH):
            print(f" â†’ local dir: {LOCAL_MODEL_PATH}")
            return SentenceTransformer(LOCAL_MODEL_PATH, device=DEVICE, trust_remote_code=True)
        print(" â†’ fall back to HF cache")
        return SentenceTransformer(MODEL_NAME, device=DEVICE, trust_remote_code=True)
    except Exception as e:
        sys.exit(f"âŒ Encoder load failed: {e}")

# ------------------------------------------------------------
#                     RETRIEVER â€“ TF-IDF
# ------------------------------------------------------------
class RetrieverTFIDF:
    IDX_PKL = "retr_tfidf.pkl"

    def __init__(self, rebuild=False, soft_prompt=""):
        self.soft_prompt = soft_prompt

        self.evidence = json.load(open(EVI_PATH))
        self.ids, self.texts = zip(*self.evidence.items())

        if rebuild or not pathlib.Path(self.IDX_PKL).exists():
            self._build_index()
        else:
            self._load_index()

        # Encoding for semantic re-ranking
        self.encoder = build_encoder()
        if rebuild or not pathlib.Path("tfidf_emb.npy").exists():
            self._encode_evidence()
        else:
            self._load_embeddings()

    # ---------- Index ----------
    def _build_index(self):
        print("[Retriever] build TF-IDF index")
        self.vectorizer = TfidfVectorizer(
            stop_words="english",
            ngram_range=(1,2),
            max_df=0.9,
            min_df=2,
            dtype=np.float32
        )
        self.evi_mat = self.vectorizer.fit_transform(self.texts)
        joblib.dump({"vec": self.vectorizer, "mat": self.evi_mat}, self.IDX_PKL)

    def _load_index(self):
        obj = joblib.load(self.IDX_PKL)
        self.vectorizer, self.evi_mat = obj["vec"], obj["mat"]

    # ---------- Evidence embedding ----------
    def _encode_evidence(self):
        print("[Retriever] encode evidence embeddings â€¦")
        embs = self.encoder.encode(
            self.texts,
            batch_size=BATCH_SIZE,
            normalize_embeddings=True,
            show_progress_bar=True
        )
        np.save("tfidf_emb.npy", embs)
        pickle.dump(self.ids, open("tfidf_ids.pkl","wb"))
        self.emb_dict = dict(zip(self.ids, embs))

    def _load_embeddings(self):
        self.ids      = pickle.load(open("tfidf_ids.pkl","rb"))
        self.emb_dict = dict(zip(self.ids, np.load("tfidf_emb.npy")))

    # ---------- Retrieve ----------
    def retrieve(self, claim:str, k:int=MAX_EVI_RET, return_scores=False):
        query = f"{self.soft_prompt} {claim}".strip()

        # â‘  TF-IDF coarse rank
        q_vec  = self.vectorizer.transform([query])
        scores = (q_vec @ self.evi_mat.T).toarray()[0]
        idx    = np.argsort(scores)[::-1][:TOPK_CANDIDATE]

        # â‘¡ MiniLM re-rank
        q_emb = self.encoder.encode(query, normalize_embeddings=True)
        reranked = [(self.ids[i], float(q_emb @ self.emb_dict[self.ids[i]]))
                    for i in idx]
        reranked.sort(key=lambda x: x[1], reverse=True)
        top = reranked[:k]

        if return_scores:
            return top
        eids = [eid for eid,_ in top]
        etxt = [self.evidence[eid] for eid in eids]
        return eids, etxt

# ------------------------------------------------------------
#                    è®­ç»ƒ / é¢„æµ‹ / è¯„ä¼°
# ------------------------------------------------------------
def train_classifier(soft_prompt):
    retr    = RetrieverTFIDF(rebuild=False, soft_prompt=soft_prompt)
    encoder = build_encoder()

    def encode_split(split):
        claims = json.load(open(f"{DATA_DIR}/{split}-claims.json"))
        X, y = [], []
        for c in tqdm.tqdm(claims.values(), desc=f"encode {split}"):
            ev = [retr.evidence[eid] for eid in c["evidences"][:MAX_EVI_RET]]
            txt = f"{soft_prompt} {c['claim_text']} </s> " + " ".join(ev)
            X.append(txt.strip())
            y.append(LABEL_MAP[c["claim_label"]])
        Xemb = encoder.encode(X, batch_size=BATCH_SIZE,
                              normalize_embeddings=False,
                              show_progress_bar=True)
        return Xemb, np.array(y)

    X, y = encode_split("train")
    clf  = LogisticRegression(max_iter=1000, multi_class="ovr")
    clf.fit(X, y)
    joblib.dump({"clf":clf, "soft_prompt":soft_prompt}, "logreg_tfidf.joblib")
    print(f"[Classifier] saved â€“ Train acc={clf.score(X,y):.4f}")

def run_split(claim_json, out_json, soft_prompt, limit=None):
    if not pathlib.Path("logreg_tfidf.joblib").exists():
        sys.exit("âŒ è¯·å…ˆè®­ç»ƒåˆ†ç±»å™¨ (--train)")
    clf_pkg = joblib.load("logreg_tfidf.joblib")
    if soft_prompt != clf_pkg["soft_prompt"]:
        warnings.warn("å½“å‰ soft-prompt ä¸è®­ç»ƒæ—¶ä¸ä¸€è‡´ï¼")

    retr = RetrieverTFIDF(rebuild=False, soft_prompt=soft_prompt)
    enc  = build_encoder()
    clf  = clf_pkg["clf"]

    claims = json.load(open(claim_json))
    if limit:
        claims = dict(list(claims.items())[:limit])

    res = {}
    for cid,c in tqdm.tqdm(claims.items(), desc="predict"):
        eids, etexts = retr.retrieve(c["claim_text"])
        if etexts:
            vec = enc.encode(f"{soft_prompt} {c['claim_text']} </s> "
                             + " ".join(etexts),
                             normalize_embeddings=False)
            label = ID2LABEL[int(clf.predict([vec])[0])]
        else:
            label = "NOT_ENOUGH_INFO"
        res[cid] = {"claim_label":label, "evidences":eids}
    json.dump(res, open(out_json,"w"), indent=2)
    print(f"[Output] {out_json} saved")

def evaluate(pred_json, gold_json):
    p, g = json.load(open(pred_json)), json.load(open(gold_json))
    f_list, acc_list = [], []
    for cid,true in g.items():
        if cid not in p: continue
        pred = p[cid]
        acc_list.append(pred["claim_label"] == true["claim_label"])
        tp = len(set(pred["evidences"]) & set(true["evidences"]))
        prec = tp/len(pred["evidences"]) if pred["evidences"] else 0
        rec  = tp/len(true["evidences"])
        f    = 0 if prec+rec==0 else 2*prec*rec/(prec+rec)
        f_list.append(f)
    F, A = np.mean(f_list), np.mean(acc_list)
    H    = 0 if F+A==0 else 2*F*A/(F+A)
    print(f"\nDev  Retrieval-F={F:.4f}  Acc={A:.4f}  Harmonic={H:.4f}\n")

# ---------- Top-K å¯¼å‡º ----------
def dump_topk(split_json, out_json, soft_prompt, k):
    retr = RetrieverTFIDF(rebuild=False, soft_prompt=soft_prompt)
    claims = json.load(open(split_json))
    out={}
    for cid,c in tqdm.tqdm(claims.items(), desc=f"Top{k}"):
        eids_scores = retr.retrieve(c["claim_text"], k=k, return_scores=True)
        out[cid] = {"evidences":[eid for eid,_ in eids_scores]}
    json.dump(out, open(out_json,"w"), indent=2)
    print(f"[TopK] {out_json} saved")

# ---------- Top-K è¯„ä¼° ----------
def eval_topk(train_claims_path: str, topk_json: str):
    train_claims = json.load(open(train_claims_path))
    topk_pred    = json.load(open(topk_json))

    recalls, precisions = [], []
    for cid, c in train_claims.items():
        gt = set(c.get("evidences", []))
        ret = set(topk_pred.get(cid, {}).get("evidences", []))
        if not gt:
            continue
        tp  = len(gt & ret)
        recalls.append(tp / len(gt))
        precisions.append(tp / len(ret) if ret else 0)

    R = sum(recalls) / len(recalls)
    P = sum(precisions) / len(precisions)
    k_val = len(next(iter(topk_pred.values()))["evidences"])

    print(f"\n=== Retrieval Top{k_val} ===")
    print(f"Average Recall   : {R:.3f}")
    print(f"Average Precision: {P:.3f}\n")

# ------------------------------------------------------------
#                        CLI
# ------------------------------------------------------------
def cli():
    ap = argparse.ArgumentParser()
    ap.add_argument("--rebuild-index", action="store_true")
    ap.add_argument("--train",         action="store_true")
    ap.add_argument("--dev-eval",      action="store_true")
    ap.add_argument("--predict-test",  action="store_true")

    # Top-K
    ap.add_argument("--dump-topk", action="store_true")
    ap.add_argument("--eval-topk", action="store_true")
    ap.add_argument("--topk", type=int, default=100)
    ap.add_argument("--split", choices=["train","dev","test"], default="dev")

    ap.add_argument("--limit", type=int)
    ap.add_argument("--soft-prompt", default="")
    return ap.parse_args()

# ------------------------------------------------------------
#                       ä¸»å…¥å£
# ------------------------------------------------------------
if __name__ == "__main__":
    # ç¦»çº¿æ¨¡å¼ï¼ˆè‹¥åœ¨çº¿å¯æ³¨é‡Šï¼‰
    os.environ["HF_HUB_OFFLINE"]       = "1"
    os.environ["TRANSFORMERS_OFFLINE"] = "1"

    args = cli()

    # (å¯é€‰) é‡å»ºç´¢å¼• / embedding
    if args.rebuild_index:
        RetrieverTFIDF(rebuild=True, soft_prompt=args.soft_prompt)

    # ------ Top-K å¯¼å‡º ------
    if args.dump_topk:
        split_file = (f"{DATA_DIR}/{args.split}-claims.json"
                      if args.split!="test"
                      else f"{DATA_DIR}/test-claims-unlabelled.json")
        out_file = f"{args.split}-claims-top{args.topk}.json"
        dump_topk(split_file, out_file, args.soft_prompt, args.topk)

    # ------ Top-K è¯„ä¼° ------
    if args.eval_topk:
        if args.split == "test":
            sys.exit("âŒ test split æ— æ ‡ç­¾ï¼Œæ— æ³•è¯„ä¼° Top-K")
        top_file = f"{args.split}-claims-top{args.topk}.json"
        if not pathlib.Path(top_file).exists():
            sys.exit(f"âŒ {top_file} ä¸å­˜åœ¨ï¼Œè¯·å…ˆ --dump-topk")
        eval_topk(f"{DATA_DIR}/{args.split}-claims.json", top_file)

    # ------ è®­ç»ƒ ------
    if args.train:
        train_classifier(args.soft_prompt)

    # ------ Dev Eval ------
    if args.dev_eval:
        run_split(f"{DATA_DIR}/dev-claims.json", "dev-preds.json",
                  args.soft_prompt, args.limit)
        evaluate("dev-preds.json", f"{DATA_DIR}/dev-claims.json")

    # ------ Test é¢„æµ‹ ------
    if args.predict_test:
        run_split(f"{DATA_DIR}/test-claims-unlabelled.json",
                  "test-claims-predictions.json",
                  args.soft_prompt, args.limit)
        print("âœ… test-claims-predictions.json ready")
