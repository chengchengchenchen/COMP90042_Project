{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32yCsRUo8H33"
      },
      "source": [
        "# 2025 COMP90042 Project\n",
        "*Make sure you change the file name with your group id.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCybYoGz8YWQ"
      },
      "source": [
        "# Readme\n",
        "## Group: 61\n",
        "\n",
        "| Name              | Student ID | Email                               |\n",
        "| ----------------- | ---------- | ----------------------------------- |\n",
        "| Jingcheng Qian    | 1640690    | jingchengq@student.unimelb.edu.au   |\n",
        "| Weichen Wang      |            |                                     |\n",
        "| Yue Zhang         |            |                                     |\n",
        "## Overview\n",
        "\n",
        "This notebook contains the essential functions required for task implementation. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6po98qVA8bJD"
      },
      "source": [
        "# 1.DataSet Processing\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\77280\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\77280\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import re, json, ujson, numpy as np\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk import download\n",
        "from rank_bm25 import BM25Okapi\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import unicodedata\n",
        "\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "from sentence_transformers import CrossEncoder\n",
        "\n",
        "download(\"punkt\")\n",
        "download(\"stopwords\")\n",
        "\n",
        "STOP = set(stopwords.words(\"english\"))\n",
        "STEM = PorterStemmer().stem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qvff21Hv8zjk"
      },
      "outputs": [],
      "source": [
        "# read evidence.json\n",
        "# note that the data file does not include evidence.json\n",
        "ev_path = Path(\"./data/evidence.json\")\n",
        "with ev_path.open(\"r\", encoding=\"utf-8\") as f:\n",
        "    evid_dict = ujson.load(f)\n",
        "\n",
        "evid_ids   = list(evid_dict.keys())\n",
        "raw_texts  = [evid_dict[eid] for eid in evid_ids]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Two-Stage Evidence Retrieval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Data clean**\n",
        "\n",
        "remove unicode and keep words and alphanumerics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def full_clean(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Perform text cleaning:\n",
        "      1. Normalizing to NFC form\n",
        "      2. Removing soft hyphens and various dash characters\n",
        "      3. Decomposing and stripping diacritics (NFKD + ASCII encoding)\n",
        "      4. Translating any remaining non-ASCII characters to ASCII (Unidecode)\n",
        "      5. Removing control characters and symbol characters\n",
        "      6. keeping words and alphanumerics \n",
        "    \"\"\"\n",
        "    txt = unicodedata.normalize(\"NFC\", text)\n",
        "    txt = re.sub(r\"[\\u00AD\\u2010-\\u2014\\-]\", \"\", txt)\n",
        "    txt = unicodedata.normalize(\"NFKD\", txt)\n",
        "    txt = txt.encode(\"ASCII\", \"ignore\").decode(\"utf-8\")\n",
        "    txt = unidecode(txt)\n",
        "    txt = regex.sub(r\"[\\p{C}\\p{S}]+\", \"\", txt)\n",
        "    txt = re.sub(r\"[^A-Za-z0-9\\s]\", \"\", txt)\n",
        "    return txt\n",
        "\n",
        "def nltk_stem_preprocessor(text: str) -> str:\n",
        "    txt = full_clean(text).lower()\n",
        "    words = txt.split()\n",
        "    stems = [STEM(w) for w in words if w not in STOP]\n",
        "    return \" \".join(stems)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### BM25 Retrieval\n",
        "\n",
        "Use BM25 for first stage evidence retrieval, generate top-100 evidences from evidence.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cv = CountVectorizer(\n",
        "        ngram_range=(1, 2),\n",
        "        preprocessor=nltk_stem_preprocessor,\n",
        "        tokenizer=lambda text: text.split(),\n",
        "        token_pattern=None,             \n",
        "        stop_words=None,\n",
        "    )\n",
        "analyzer = cv.build_analyzer()\n",
        "\n",
        "# TOKEN\n",
        "token_corpus = [analyzer(doc) for doc in tqdm(raw_texts, desc=\"Tokenize\")]\n",
        "\n",
        "bm25 = BM25Okapi(token_corpus, k1=1.2, b=0.75)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# query claim and return Top-k relevant evidence\n",
        "def retrieve_topk(claim_text: str, topk: int = 100):\n",
        "    query_tokens = analyzer(claim_text)\n",
        "    scores       = bm25.get_scores(query_tokens)\n",
        "    idx_sorted   = np.argsort(scores)[-topk:][::-1]\n",
        "    return [(evid_ids[i], float(scores[i])) for i in idx_sorted]\n",
        "\n",
        "# batch process\n",
        "def process_claim_file(claim_json: str, out_json: str):\n",
        "    with open(claim_json, \"r\", encoding=\"utf-8\") as f:\n",
        "        claims = json.load(f)            # {claim_id: {...}}\n",
        "    results = {}\n",
        "    for cid, obj in tqdm(claims.items(), desc=\"Retrieve\"):\n",
        "        hits = retrieve_topk(obj[\"claim_text\"])\n",
        "        results[cid] = {\"evidences\": [h[0] for h in hits]}\n",
        "    with open(out_json, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(results, f, ensure_ascii=False, indent=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For efficiency, comment the batch process function. Uncomment these to test the output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# process_claim_file(\"./data/train-claims.json\", \"./data/train-claims-top100.json\")\n",
        "# process_claim_file(\"./data/dev-claims.json\", \"./data/dev-claims-top100.json\")\n",
        "# process_claim_file(\"./data/test-claims-unlabelled.json\", \"./data/test-claims-top100.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cross-Encoder Reranker\n",
        "Reads top-100 candidates per claim, re-scores them with a pretrained CrossEncoder, and emits the top-M evidences in both ID form and full-text form."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[dev] Loading top-100 lists…\n",
            "[dev] Reranking with Cross-Encoder…\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "dev split: 100%|██████████| 154/154 [00:16<00:00,  9.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[dev] → wrote dev-claims-top6-dense-ce.json and dev-claims-top6-text-ce.json\n",
            "[test] Loading top-100 lists…\n",
            "[test] Reranking with Cross-Encoder…\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "test split: 100%|██████████| 153/153 [00:16<00:00,  9.36it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[test] → wrote test-claims-top6-dense-ce.json and test-claims-top6-text-ce.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ---------- Config ----------\n",
        "DATA_DIR   = Path(\"data\")\n",
        "TOP100_FNS = {\n",
        "    \"train\": \"train-claims-top100.json\",\n",
        "    \"dev\"  : \"dev-claims-top100.json\",\n",
        "    \"test\" : \"test-claims-top100.json\"\n",
        "}\n",
        "TOP_M      = 6\n",
        "MODEL_NAME = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "\n",
        "# Initialize Cross-Encoder\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "ce_model = CrossEncoder(MODEL_NAME, device=device)\n",
        "\n",
        "# ---------- Process each split ----------\n",
        "for split, fn in TOP100_FNS.items():\n",
        "    top100_path = DATA_DIR / fn\n",
        "    if not top100_path.exists():\n",
        "        continue\n",
        "\n",
        "    print(f\"[{split}] Loading top-100 lists…\")\n",
        "    with top100_path.open() as f:\n",
        "        top100 = ujson.load(f)\n",
        "\n",
        "    # Load raw claim texts\n",
        "    cfile = DATA_DIR / f\"{split}-claims.json\"\n",
        "    with cfile.open() as f:\n",
        "        raw = ujson.load(f)\n",
        "    claim_texts = {\n",
        "        cid: (raw[cid][\"claim_text\"] if isinstance(raw[cid], dict) else raw[cid])\n",
        "        for cid in raw\n",
        "    }\n",
        "\n",
        "    dense_out = {}\n",
        "    text_out  = {}\n",
        "\n",
        "    print(f\"[{split}] Reranking with Cross-Encoder…\")\n",
        "    for cid, entry in tqdm(top100.items(), desc=f\"{split} split\"):\n",
        "        cand_ids = entry[\"evidences\"] if isinstance(entry, dict) else entry\n",
        "        claim    = claim_texts.get(cid, \"\")\n",
        "\n",
        "        # Build (claim, evidence) pairs\n",
        "        pairs = [(claim, evid_dict[eid]) for eid in cand_ids]\n",
        "\n",
        "        # Score\n",
        "        scores = ce_model.predict(pairs, batch_size=BATCH_SIZE)\n",
        "\n",
        "        # Pick top-M\n",
        "        top_idx = scores.argsort()[-TOP_M:][::-1]\n",
        "        top_ids = [cand_ids[i] for i in top_idx]\n",
        "\n",
        "        # --- dense (just IDs) ---\n",
        "        dense_out[cid] = top_ids\n",
        "\n",
        "        # --- text (with full evidence text + claim_text) ---\n",
        "        text_out[cid] = {\n",
        "            \"claim_text\": claim,\n",
        "            \"ranked_evidences\": [\n",
        "                {\"id\": eid, \"text\": evid_dict[eid]}\n",
        "                for eid in top_ids\n",
        "            ]\n",
        "        }\n",
        "\n",
        "    # Write outputs\n",
        "    out_dense_path = DATA_DIR / f\"{split}-claims-top{TOP_M}-dense-ce.json\"\n",
        "    out_text_path  = DATA_DIR / f\"{split}-claims-top{TOP_M}-text-ce.json\"\n",
        "\n",
        "    out_dense_path.write_text(\n",
        "        json.dumps(dense_out, ensure_ascii=False, indent=2),\n",
        "        encoding=\"utf-8\"\n",
        "    )\n",
        "    out_text_path.write_text(\n",
        "        json.dumps(text_out, ensure_ascii=False, indent=2),\n",
        "        encoding=\"utf-8\"\n",
        "    )\n",
        "\n",
        "    print(f\"[{split}] → wrote {out_dense_path.name} and {out_text_path.name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FA2ao2l8hOg"
      },
      "source": [
        "# 2. Model Implementation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "# ———— Config ————\n",
        "TEST_CLAIMS_FILE = './data2/test-claims-top10-text-ce.json'\n",
        "RESULTS_FILE     = 'results.json'\n",
        "CHECKPOINT_FILE  = 'checkpoint.json'\n",
        "MODEL_NAME       = 'deepseek-ai/DeepSeek-R1-Distill-Qwen-7B'\n",
        "\n",
        "# ———— Few-shot Example ————\n",
        "FEW_SHOT_EXAMPLE = {\n",
        "    \"claim-2152\": {\n",
        "        \"claim_text\": \"Venus doesn't have a runaway greenhouse effect\",\n",
        "        \"ranked_evidences\": [\n",
        "            {\n",
        "                \"id\": \"evidence-1018575\",\n",
        "                \"text\": (\n",
        "                    \"A runaway greenhouse effect involving carbon dioxide and water vapor \"\n",
        "                    \"has long ago been hypothesized to have occurred on Venus, this idea \"\n",
        "                    \"is still largely accepted.\"\n",
        "                )\n",
        "            },\n",
        "            {\n",
        "                \"id\": \"evidence-791159\",\n",
        "                \"text\": (\n",
        "                    \"Venus receives about twice the sunlight that Earth does, which is \"\n",
        "                    \"thought to have contributed to its runaway greenhouse effect.\"\n",
        "                )\n",
        "            },\n",
        "            {\n",
        "                \"id\": \"evidence-500249\",\n",
        "                \"text\": (\n",
        "                    \"In the extreme, the planet Venus is thought to have experienced a \"\n",
        "                    \"very large increase in greenhouse effect over its lifetime, so much \"\n",
        "                    \"so that its poles have warmed sufficiently to render its surface \"\n",
        "                    \"temperature effectively isothermal.\"\n",
        "                )\n",
        "            }\n",
        "        ],\n",
        "        \"claim_label\": \"REFUTES\",\n",
        "        \"evidences\": [\"evidence-1018575\", \"evidence-791159\"]\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ———— Helpers ————\n",
        "def load_json(path, default):\n",
        "    if os.path.isfile(path):\n",
        "        with open(path, 'r', encoding='utf-8') as f:\n",
        "            return json.load(f)\n",
        "    return default\n",
        "\n",
        "def save_json(obj, path):\n",
        "    with open(path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(obj, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "def build_prompt(claim_id, claim_obj):\n",
        "    lines = []\n",
        "    # system/user instruction\n",
        "    lines.append(\n",
        "        \"You are a fact-checking assistant. \"\n",
        "        \"For the given Claim and Candidate Evidences, determine the correct Claim Label \"\n",
        "        \"and list the IDs of those evidences you deem relevant, at least one evidence. \"\n",
        "        \"The label is one of [SUPPORTS, REFUTES, NOT_ENOUGH_INFO, DISPUTED].\"\n",
        "        '''\n",
        "        Classification definitions:\n",
        "        - SUPPORTS: The evidence directly confirms the claim is true\n",
        "        - REFUTES: The evidence directly contradicts the claim, showing it's false\n",
        "        - DISPUTED: The evidence contains conflicting information about the claim\n",
        "        - NOT_ENOUGH_INFO: The evidence is insufficient to make a determination\n",
        "        '''\n",
        "    )\n",
        "\n",
        "    # few-shot block\n",
        "    for ex_id, ex in FEW_SHOT_EXAMPLE.items():\n",
        "        lines.append(f'\"{ex_id}\": \"{ex[\"claim_text\"]}\",')\n",
        "        lines.append(\"  \\\"ranked_evidences\\\": [\")\n",
        "        for ev in ex[\"ranked_evidences\"]:\n",
        "            lines.append(f'    {{\"{ev[\"id\"]}\": \"{ev[\"text\"]}\"}},')\n",
        "        example_label_evidences = {\n",
        "            \"label\": ex[\"claim_label\"],\n",
        "            \"evidences\": ex[\"evidences\"]\n",
        "        }\n",
        "        json_line = json.dumps(example_label_evidences)\n",
        "        lines.append(f'  {json_line},')\n",
        "        lines.append(\"\")  # separator\n",
        "\n",
        "    # target claim\n",
        "    lines.append(\n",
        "    \"Now, please output **only** valid JSON, with exactly these two keys:\\n\"\n",
        "    \"  \\\"label\\\": string,\\n\"\n",
        "    \"  \\\"evidences\\\": an array of evidence ID strings (e.g. [\\\"evidence-123\\\",\\\"evidence-456\\\"]).\"\n",
        "    )\n",
        "    lines.append(f'\"{claim_id}\": \"{claim_obj[\"claim_text\"]}\",')\n",
        "    lines.append(\"  \\\"evidences\\\": [\")\n",
        "    for ev in claim_obj[\"ranked_evidences\"]:\n",
        "        lines.append(f'    {{\"{ev[\"id\"]}\": \"{ev[\"text\"]}\"}},')\n",
        "    lines.append(\"  ]\")\n",
        "    lines.append(\"label:\")\n",
        "    lines.append(\"evidences:\")\n",
        "    return \"\\n\".join(lines)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load Model\n",
        "load in 4-bit 7B model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B, need around 7GB GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,                       # 4 bit\n",
        "    bnb_4bit_quant_type=\"nf4\",               \n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,   # bf16\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"cuda\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Predict\n",
        "use checkpoint to store model state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ———— Main Loop ————\n",
        "def main():\n",
        "    test_data   = load_json(TEST_CLAIMS_FILE, {})\n",
        "    results     = load_json(RESULTS_FILE, {})\n",
        "    checkpoint  = load_json(CHECKPOINT_FILE, {\"last_id\": None})\n",
        "    started = checkpoint[\"last_id\"] is None\n",
        "\n",
        "    for cid, claim in tqdm(test_data.items(), desc=\"Claims\"):\n",
        "        # skip until after last checkpoint\n",
        "        if not started:\n",
        "            if cid == checkpoint[\"last_id\"]:\n",
        "                started = True\n",
        "            continue\n",
        "\n",
        "        prompt = build_prompt(cid, claim)\n",
        "\n",
        "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "        text = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True,\n",
        "            enable_thinking=True\n",
        "        )\n",
        "\n",
        "        model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "        # conduct text completion\n",
        "        generated_ids = model.generate(\n",
        "            **model_inputs,\n",
        "            max_new_tokens=32768\n",
        "        )\n",
        "        output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n",
        "        content = tokenizer.decode(output_ids[0], skip_special_tokens=True).strip(\"\\n\")\n",
        "\n",
        "\n",
        "        # parse JSON\n",
        "        try:\n",
        "            # clean\n",
        "            m = re.search(r'(\\{.*\\})', content, flags=re.DOTALL)\n",
        "\n",
        "            json_str = m.group(1)\n",
        "\n",
        "            parsed = json.loads(json_str)\n",
        "            results[cid] = {\n",
        "                \"claim_label\": parsed[\"label\"],\n",
        "                \"evidences\":   parsed[\"evidences\"]\n",
        "            }\n",
        "\n",
        "        # retru if output is not json format\n",
        "        except json.JSONDecodeError:\n",
        "            print(f\"[WARN] JSON parse failed for {cid}, try again.\")\n",
        "            continue\n",
        "        except AttributeError:\n",
        "            continue\n",
        "\n",
        "        # persist\n",
        "        save_json(results, RESULTS_FILE)\n",
        "        checkpoint[\"last_id\"] = cid\n",
        "        save_json(checkpoint, CHECKPOINT_FILE)\n",
        "\n",
        "        # pause\n",
        "        time.sleep(0.5)\n",
        "\n",
        "    print(\"All done.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzGuzHPE87Ya"
      },
      "source": [
        "# 3.Testing and Evaluation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Compute Recall and F-score to evaluate evidence retrieval output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZVeNYIH9IaL"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import argparse\n",
        "from typing import List, Tuple, Optional\n",
        "\n",
        "def compute_metrics(\n",
        "    gt: List[str],\n",
        "    retrieved: List[str]\n",
        ") -> Tuple[int, Optional[float], Optional[float]]:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        gt: ground-truth evidence ID\n",
        "        retrieved: top-100 evidence ID \n",
        "\n",
        "    Returns:\n",
        "        tp: hit\n",
        "        recall: tp / len(gt) \n",
        "        precision: tp / len(retrieved)\n",
        "    \"\"\"\n",
        "    set_gt = set(gt)\n",
        "    set_ret = set(retrieved)\n",
        "    tp = len(set_gt & set_ret)\n",
        "    recall = tp / len(gt) if gt else None\n",
        "    precision = tp / len(retrieved) if retrieved else None\n",
        "    return tp, recall, precision\n",
        "\n",
        "def main(train_claims_path: str, top100_path: str):\n",
        "    # read json\n",
        "    with open(train_claims_path, 'r', encoding='utf-8') as f:\n",
        "        train_claims = json.load(f)\n",
        "    with open(top100_path, 'r', encoding='utf-8') as f:\n",
        "        top100 = json.load(f)\n",
        "\n",
        "    recalls = []\n",
        "    precisions = []\n",
        "\n",
        "    # each claim\n",
        "    for claim_id, claim_info in train_claims.items():\n",
        "        gt_list = claim_info.get(\"evidences\", [])\n",
        "        retrieved_list = top100.get(claim_id, {})\n",
        "\n",
        "        tp, recall, precision = compute_metrics(gt_list, retrieved_list)\n",
        "        recalls.append(recall if recall is not None else 0.0)\n",
        "        precisions.append(precision if precision is not None else 0.0)\n",
        "\n",
        "    # avg\n",
        "    avg_recall = sum(recalls) / len(recalls) if recalls else 0.0\n",
        "    avg_precision = sum(precisions) / len(precisions) if precisions else 0.0\n",
        "    if (avg_precision + avg_recall) > 0:\n",
        "        avg_f1 = 2 * avg_precision * avg_recall / (avg_precision + avg_recall)\n",
        "    else:\n",
        "        avg_f1 = 0.0\n",
        "    print(\"\\n=== Overall ===\")\n",
        "    print(f\"Average Recall@k   : {avg_recall:.3f}\")\n",
        "    print(f\"Average Precision@k: {avg_precision:.3f}\")\n",
        "    print(f\"Average F1@k       : {avg_f1:.3f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description=\"topk Recall Precision F-score\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--train_claims\",\n",
        "        type=str,\n",
        "        default=\"./data/dev-claims.json\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--top100\",\n",
        "        type=str,\n",
        "        default=\"./data/dev-claims-top10-dense.json\",\n",
        "    )\n",
        "    args = parser.parse_args()\n",
        "    main(args.train_claims, args.top100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mefSOe8eTmGP"
      },
      "source": [
        "## Object Oriented Programming codes here\n",
        "\n",
        "*You can use multiple code snippets. Just add more if needed*"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "nlp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
