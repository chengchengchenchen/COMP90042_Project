{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32yCsRUo8H33"
      },
      "source": [
        "# 2025 COMP90042 Project\n",
        "*Make sure you change the file name with your group id.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCybYoGz8YWQ"
      },
      "source": [
        "# Readme\n",
        "## Group: 61\n",
        "\n",
        "| Name              | Student ID | Email                               |\n",
        "| ----------------- | ---------- | ----------------------------------- |\n",
        "| Jingcheng Qian    | 1640690    | jingchengq@student.unimelb.edu.au   |\n",
        "| Weichen Wang      |            |                                     |\n",
        "| Yue Zhang         |            |                                     |\n",
        "## Overview\n",
        "\n",
        "This notebook contains the essential functions required for task implementation. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6po98qVA8bJD"
      },
      "source": [
        "# 1.DataSet Processing\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\77280\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\77280\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import re, json, ujson, numpy as np\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk import download\n",
        "from rank_bm25 import BM25Okapi\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import unicodedata\n",
        "\n",
        "\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "from sentence_transformers import CrossEncoder\n",
        "\n",
        "download(\"punkt\")\n",
        "download(\"stopwords\")\n",
        "\n",
        "STOP = set(stopwords.words(\"english\"))\n",
        "STEM = PorterStemmer().stem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qvff21Hv8zjk"
      },
      "outputs": [],
      "source": [
        "# read evidence.json\n",
        "# note that the data file does not include evidence.json\n",
        "ev_path = Path(\"./data/evidence.json\")\n",
        "with ev_path.open(\"r\", encoding=\"utf-8\") as f:\n",
        "    evid_dict = ujson.load(f)\n",
        "\n",
        "evid_ids   = list(evid_dict.keys())\n",
        "raw_texts  = [evid_dict[eid] for eid in evid_ids]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Two-Stage Evidence Retrieval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data clean\n",
        "\n",
        "remove unicode and keep words and numbers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from unidecode import unidecode\n",
        "import regex\n",
        "\n",
        "def full_clean(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Perform text cleaning:\n",
        "      1. Normalizing to NFC form\n",
        "      2. Removing soft hyphens and various dash characters\n",
        "      3. Decomposing and stripping diacritics (NFKD + ASCII encoding)\n",
        "      4. Translating any remaining non-ASCII characters to ASCII (Unidecode)\n",
        "      5. Removing control characters and symbol characters\n",
        "      6. keeping words and alphanumerics \n",
        "    \"\"\"\n",
        "    txt = unicodedata.normalize(\"NFC\", text)\n",
        "    txt = re.sub(r\"[\\u00AD\\u2010-\\u2014\\-]\", \"\", txt)\n",
        "    txt = unicodedata.normalize(\"NFKD\", txt)\n",
        "    txt = txt.encode(\"ASCII\", \"ignore\").decode(\"utf-8\")\n",
        "    txt = unidecode(txt)\n",
        "    txt = regex.sub(r\"[\\p{C}\\p{S}]+\", \"\", txt)\n",
        "    txt = re.sub(r\"[^A-Za-z0-9\\s]\", \"\", txt)\n",
        "    return txt\n",
        "\n",
        "def nltk_stem_preprocessor(text: str) -> str:\n",
        "    txt = full_clean(text).lower()\n",
        "    words = txt.split()\n",
        "    stems = [STEM(w) for w in words if w not in STOP]\n",
        "    return \" \".join(stems)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### BM25 Retrieval\n",
        "\n",
        "Use BM25 for first stage evidence retrieval, generate top-100 evidences from evidence.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenize: 100%|██████████| 1208827/1208827 [04:17<00:00, 4701.09it/s]\n"
          ]
        }
      ],
      "source": [
        "cv = CountVectorizer(\n",
        "        ngram_range=(1, 2),\n",
        "        preprocessor=nltk_stem_preprocessor,\n",
        "        tokenizer=lambda text: text.split(),\n",
        "        token_pattern=None,             \n",
        "        stop_words=None,\n",
        "    )\n",
        "analyzer = cv.build_analyzer()\n",
        "\n",
        "# TOKEN\n",
        "token_corpus = [analyzer(doc) for doc in tqdm(raw_texts, desc=\"Tokenize\")]\n",
        "\n",
        "bm25 = BM25Okapi(token_corpus, k1=1.2, b=0.75)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Length Category   Count  Percentage (%)\n",
            "0       ≤ 5 words   11343            0.94\n",
            "1      6-10 words  106205            8.79\n",
            "2     11-20 words  465619           38.52\n",
            "3     21-50 words  584593           48.36\n",
            "4    51-100 words   40052            3.31\n",
            "5   101-200 words     971            0.08\n",
            "6      >200 words      44            0.00\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "token_lengths = [len(tokens) for tokens in token_corpus]\n",
        "bins = [0, 5, 10, 20, 50, 100, 200, float('inf')]\n",
        "labels = [\n",
        "    \"≤ 5 words\",\n",
        "    \"6-10 words\",\n",
        "    \"11-20 words\",\n",
        "    \"21-50 words\",\n",
        "    \"51-100 words\",\n",
        "    \"101-200 words\",\n",
        "    \">200 words\"\n",
        "]\n",
        "\n",
        "# divide\n",
        "cut_series = pd.cut(token_lengths, bins=bins, labels=labels, include_lowest=True)\n",
        "counts = cut_series.value_counts().reindex(labels).fillna(0).astype(int)\n",
        "\n",
        "# percentage\n",
        "total = counts.sum()\n",
        "percentages = (counts / total * 100).round(2)\n",
        "\n",
        "# result\n",
        "distribution_df = pd.DataFrame({\n",
        "    \"Length Category\": labels,\n",
        "    \"Count\": counts.values,\n",
        "    \"Percentage (%)\": percentages.values\n",
        "})\n",
        "\n",
        "# output\n",
        "print(distribution_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1208827\n"
          ]
        }
      ],
      "source": [
        "print(len(token_corpus))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "# query claim and return Top-k relevant evidence\n",
        "def retrieve_topk(claim_text: str, topk: int = 100):\n",
        "    query_tokens = analyzer(claim_text)\n",
        "    scores       = bm25.get_scores(query_tokens)\n",
        "    idx_sorted   = np.argsort(scores)[-topk:][::-1]\n",
        "    return [(evid_ids[i], float(scores[i])) for i in idx_sorted]\n",
        "\n",
        "# batch process\n",
        "def process_claim_file(claim_json: str, out_json: str):\n",
        "    with open(claim_json, \"r\", encoding=\"utf-8\") as f:\n",
        "        claims = json.load(f)            # {claim_id: {...}}\n",
        "    results = {}\n",
        "    for cid, obj in tqdm(claims.items(), desc=\"Retrieve\"):\n",
        "        hits = retrieve_topk(obj[\"claim_text\"])\n",
        "        results[cid] = {\"evidences\": [h[0] for h in hits]}\n",
        "    with open(out_json, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(results, f, ensure_ascii=False, indent=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For efficiency, comment the batch process function. Uncomment these to test the output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Retrieve: 100%|██████████| 1228/1228 [4:38:53<00:00, 13.63s/it]  \n"
          ]
        }
      ],
      "source": [
        "process_claim_file(\"./data/train-claims.json\", \"./data/train-claims-top100.json\")\n",
        "# process_claim_file(\"./data/dev-claims.json\", \"./data/dev-claims-top100.json\")\n",
        "# process_claim_file(\"./data/test-claims-unlabelled.json\", \"./data/test-claims-top100.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cross-Encoder Reranker\n",
        "Reads top-100 candidates per claim, re-scores them with a pretrained CrossEncoder, and emits the top-M evidences in both ID form and full-text form.\n",
        "\n",
        "The full-text form will be used for prompt building"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------- Config ----------\n",
        "DATA_DIR   = Path(\"data\")\n",
        "TOP100_FNS = {\n",
        "    \"train\": \"train-claims-top100.json\",\n",
        "    \"dev\"  : \"dev-claims-top100.json\",\n",
        "    \"test\" : \"test-claims-top100.json\"\n",
        "}\n",
        "TOP_M      = 6\n",
        "MODEL_NAME = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "\n",
        "# Initialize Cross-Encoder\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "ce_model = CrossEncoder(MODEL_NAME, device=device)\n",
        "\n",
        "# ---------- Process each split ----------\n",
        "for split, fn in TOP100_FNS.items():\n",
        "    top100_path = DATA_DIR / fn\n",
        "    if not top100_path.exists():\n",
        "        continue\n",
        "\n",
        "    print(f\"[{split}] Loading top-100 lists…\")\n",
        "    with top100_path.open() as f:\n",
        "        top100 = ujson.load(f)\n",
        "\n",
        "    # Load raw claim texts\n",
        "    cfile = DATA_DIR / f\"{split}-claims.json\"\n",
        "    with cfile.open() as f:\n",
        "        raw = ujson.load(f)\n",
        "    claim_texts = {\n",
        "        cid: (raw[cid][\"claim_text\"] if isinstance(raw[cid], dict) else raw[cid])\n",
        "        for cid in raw\n",
        "    }\n",
        "\n",
        "    dense_out = {}\n",
        "    text_out  = {}\n",
        "\n",
        "    print(f\"[{split}] Reranking with Cross-Encoder…\")\n",
        "    for cid, entry in tqdm(top100.items(), desc=f\"{split} split\"):\n",
        "        cand_ids = entry[\"evidences\"] if isinstance(entry, dict) else entry\n",
        "        claim    = claim_texts.get(cid, \"\")\n",
        "\n",
        "        # Build (claim, evidence) pairs\n",
        "        pairs = [(claim, evid_dict[eid]) for eid in cand_ids]\n",
        "\n",
        "        # Score\n",
        "        scores = ce_model.predict(pairs, batch_size=BATCH_SIZE)\n",
        "\n",
        "        # Pick top-M\n",
        "        top_idx = scores.argsort()[-TOP_M:][::-1]\n",
        "        top_ids = [cand_ids[i] for i in top_idx]\n",
        "\n",
        "        # --- dense (just IDs) ---\n",
        "        dense_out[cid] = top_ids\n",
        "\n",
        "        # --- text (with full evidence text + claim_text) ---\n",
        "        text_out[cid] = {\n",
        "            \"claim_text\": claim,\n",
        "            \"ranked_evidences\": [\n",
        "                {\"id\": eid, \"text\": evid_dict[eid]}\n",
        "                for eid in top_ids\n",
        "            ]\n",
        "        }\n",
        "\n",
        "    # Write outputs\n",
        "    out_dense_path = DATA_DIR / f\"{split}-claims-top{TOP_M}-dense-ce.json\"\n",
        "    out_text_path  = DATA_DIR / f\"{split}-claims-top{TOP_M}-text-ce.json\"\n",
        "\n",
        "    out_dense_path.write_text(\n",
        "        json.dumps(dense_out, ensure_ascii=False, indent=2),\n",
        "        encoding=\"utf-8\"\n",
        "    )\n",
        "    out_text_path.write_text(\n",
        "        json.dumps(text_out, ensure_ascii=False, indent=2),\n",
        "        encoding=\"utf-8\"\n",
        "    )\n",
        "\n",
        "    print(f\"[{split}] → wrote {out_dense_path.name} and {out_text_path.name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FA2ao2l8hOg"
      },
      "source": [
        "# 2. Model Implementation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Config && Few-shot Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "# ———— Config ————\n",
        "TEST_CLAIMS_FILE = './data/test-claims-top6-text-fce.json'\n",
        "RESULTS_FILE     = 'results.json'\n",
        "CHECKPOINT_FILE  = 'checkpoint.json'\n",
        "MODEL_NAME       = 'deepseek-ai/DeepSeek-R1-Distill-Qwen-7B'\n",
        "\n",
        "# ———— Few-shot Example ————\n",
        "ONE_SHOT_EXAMPLE = {\n",
        "    \"claim-2152\": {\n",
        "        \"claim_text\": \"Venus doesn't have a runaway greenhouse effect\",\n",
        "        \"ranked_evidences\": [\n",
        "            {\n",
        "                \"id\": \"evidence-1018575\",\n",
        "                \"text\": (\n",
        "                    \"A runaway greenhouse effect involving carbon dioxide and water vapor \"\n",
        "                    \"has long ago been hypothesized to have occurred on Venus, this idea \"\n",
        "                    \"is still largely accepted.\"\n",
        "                )\n",
        "            },\n",
        "            {\n",
        "                \"id\": \"evidence-791159\",\n",
        "                \"text\": (\n",
        "                    \"Venus receives about twice the sunlight that Earth does, which is \"\n",
        "                    \"thought to have contributed to its runaway greenhouse effect.\"\n",
        "                )\n",
        "            },\n",
        "            {\n",
        "                \"id\": \"evidence-500249\",\n",
        "                \"text\": (\n",
        "                    \"In the extreme, the planet Venus is thought to have experienced a \"\n",
        "                    \"very large increase in greenhouse effect over its lifetime, so much \"\n",
        "                    \"so that its poles have warmed sufficiently to render its surface \"\n",
        "                    \"temperature effectively isothermal.\"\n",
        "                )\n",
        "            }\n",
        "        ],\n",
        "        \"claim_label\": \"REFUTES\",\n",
        "        \"evidences\": [\"evidence-1018575\", \"evidence-791159\"]\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper function\n",
        "def load_json(path, default):\n",
        "    if os.path.isfile(path):\n",
        "        with open(path, 'r', encoding='utf-8') as f:\n",
        "            return json.load(f)\n",
        "    return default\n",
        "\n",
        "def save_json(obj, path):\n",
        "    with open(path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(obj, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "def build_prompt(claim_id, claim_obj):\n",
        "    lines = []\n",
        "    # user instruction\n",
        "    lines.append(\n",
        "        \"You are a fact-checking assistant. \"\n",
        "        \"For the given Claim and Candidate Evidences, determine the correct Claim Label \"\n",
        "        \"and list the IDs of those evidences you deem relevant, at least one evidence. \"\n",
        "        \"The label is one of [SUPPORTS, REFUTES, NOT_ENOUGH_INFO, DISPUTED].\"\n",
        "        '''\n",
        "        Classification definitions:\n",
        "        - SUPPORTS: The evidence directly confirms the claim is true\n",
        "        - REFUTES: The evidence directly contradicts the claim, showing it's false\n",
        "        - DISPUTED: The evidence contains conflicting information about the claim\n",
        "        - NOT_ENOUGH_INFO: The evidence is insufficient to make a determination\n",
        "        '''\n",
        "    )\n",
        "\n",
        "    # few-shot block\n",
        "    for ex_id, ex in ONE_SHOT_EXAMPLE.items():\n",
        "        lines.append(f'\"{ex_id}\": \"{ex[\"claim_text\"]}\",')\n",
        "        lines.append(\"  \\\"ranked_evidences\\\": [\")\n",
        "        for ev in ex[\"ranked_evidences\"]:\n",
        "            lines.append(f'    {{\"{ev[\"id\"]}\": \"{ev[\"text\"]}\"}},')\n",
        "        example_label_evidences = {\n",
        "            \"label\": ex[\"claim_label\"],\n",
        "            \"evidences\": ex[\"evidences\"]\n",
        "        }\n",
        "        json_line = json.dumps(example_label_evidences)\n",
        "        lines.append(f'  {json_line},')\n",
        "        lines.append(\"\")  # separator\n",
        "\n",
        "    # target claim\n",
        "    lines.append(\n",
        "    \"Now, please output **only** valid JSON, with exactly these two keys:\\n\"\n",
        "    \"  \\\"label\\\": string,\\n\"\n",
        "    \"  \\\"evidences\\\": an array of evidence ID strings (e.g. [\\\"evidence-123\\\",\\\"evidence-456\\\"]).\"\n",
        "    )\n",
        "    lines.append(f'\"{claim_id}\": \"{claim_obj[\"claim_text\"]}\",')\n",
        "    lines.append(\"  \\\"evidences\\\": [\")\n",
        "    for ev in claim_obj[\"ranked_evidences\"]:\n",
        "        lines.append(f'    {{\"{ev[\"id\"]}\": \"{ev[\"text\"]}\"}},')\n",
        "    lines.append(\"  ]\")\n",
        "    lines.append(\"label:\")\n",
        "    lines.append(\"evidences:\")\n",
        "    return \"\\n\".join(lines)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load Model\n",
        "load in 4-bit 7B model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B, need around 7GB GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "957140bda3d54a3ca1a9578bfd8e8c31",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,                       # 4 bit\n",
        "    bnb_4bit_quant_type=\"nf4\",               \n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,   # bf16\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"cuda\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Predict\n",
        "use checkpoint to store model state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ———— Main Loop ————\n",
        "def main():\n",
        "    test_data   = load_json(TEST_CLAIMS_FILE, {})\n",
        "    results     = load_json(RESULTS_FILE, {})\n",
        "    checkpoint  = load_json(CHECKPOINT_FILE, {\"last_id\": None})\n",
        "    started = checkpoint[\"last_id\"] is None\n",
        "\n",
        "    for cid, claim in tqdm(test_data.items(), desc=\"Claims\"):\n",
        "        # skip until after last checkpoint\n",
        "        if not started:\n",
        "            if cid == checkpoint[\"last_id\"]:\n",
        "                started = True\n",
        "            continue\n",
        "\n",
        "        prompt = build_prompt(cid, claim)\n",
        "\n",
        "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "        text = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True,\n",
        "            enable_thinking=True\n",
        "        )\n",
        "\n",
        "        model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "        # conduct text completion\n",
        "        generated_ids = model.generate(\n",
        "            **model_inputs,\n",
        "            max_new_tokens=32768\n",
        "        )\n",
        "        output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n",
        "        content = tokenizer.decode(output_ids[0:], skip_special_tokens=True).strip(\"\\n\")\n",
        "\n",
        "        # parse JSON\n",
        "        try:\n",
        "            # clean\n",
        "            m = re.search(r'(\\{.*\\})', content, flags=re.DOTALL)\n",
        "\n",
        "            json_str = m.group(1)\n",
        "\n",
        "            parsed = json.loads(json_str)\n",
        "            results[cid] = {\n",
        "                \"claim_label\": parsed[\"label\"],\n",
        "                \"evidences\":   parsed[\"evidences\"]\n",
        "            }\n",
        "\n",
        "        # return if output is not json format\n",
        "        except json.JSONDecodeError:\n",
        "            print(f\"[WARN] JSON parse failed for {cid}, try again.\")\n",
        "            continue\n",
        "        except AttributeError:\n",
        "            print(f\"No attribute, try again.\")\n",
        "            continue\n",
        "        \n",
        "        \n",
        "        # persist\n",
        "        save_json(results, RESULTS_FILE)\n",
        "        checkpoint[\"last_id\"] = cid\n",
        "        save_json(checkpoint, CHECKPOINT_FILE)\n",
        "\n",
        "        # pause\n",
        "        time.sleep(0.5)\n",
        "\n",
        "    print(\"All done.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Claims:   0%|          | 0/153 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:   8%|▊         | 12/153 [00:34<06:42,  2.85s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:   8%|▊         | 13/153 [01:26<18:52,  8.09s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:   9%|▉         | 14/153 [02:08<29:06, 12.56s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  10%|▉         | 15/153 [02:59<43:08, 18.75s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  10%|█         | 16/153 [04:15<1:06:37, 29.18s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No attribute, try again.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Claims:  11%|█         | 17/153 [04:50<1:09:16, 30.57s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  12%|█▏        | 18/153 [05:23<1:09:34, 30.92s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  12%|█▏        | 19/153 [05:59<1:11:53, 32.19s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  13%|█▎        | 20/153 [06:48<1:21:12, 36.63s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  14%|█▎        | 21/153 [07:41<1:30:11, 41.00s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  14%|█▍        | 22/153 [08:08<1:20:58, 37.09s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  15%|█▌        | 23/153 [08:53<1:25:26, 39.44s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  16%|█▌        | 24/153 [09:32<1:24:48, 39.45s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  16%|█▋        | 25/153 [10:31<1:36:23, 45.19s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  17%|█▋        | 26/153 [11:16<1:35:24, 45.07s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  18%|█▊        | 27/153 [12:00<1:33:42, 44.62s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  18%|█▊        | 28/153 [12:53<1:38:33, 47.31s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  19%|█▉        | 29/153 [13:39<1:36:30, 46.70s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  20%|█▉        | 30/153 [14:27<1:36:49, 47.23s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  20%|██        | 31/153 [15:26<1:43:08, 50.73s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  21%|██        | 32/153 [15:43<1:21:54, 40.62s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  22%|██▏       | 33/153 [16:35<1:28:15, 44.13s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  22%|██▏       | 34/153 [17:39<1:39:06, 49.97s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  23%|██▎       | 35/153 [18:17<1:31:05, 46.32s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  24%|██▎       | 36/153 [19:09<1:33:53, 48.15s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  24%|██▍       | 37/153 [20:02<1:36:03, 49.69s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  25%|██▍       | 38/153 [21:03<1:41:36, 53.01s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  25%|██▌       | 39/153 [21:47<1:35:36, 50.32s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  26%|██▌       | 40/153 [22:19<1:24:02, 44.62s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  27%|██▋       | 41/153 [22:53<1:17:47, 41.67s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  27%|██▋       | 42/153 [23:38<1:18:42, 42.54s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  28%|██▊       | 43/153 [24:22<1:18:55, 43.05s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  29%|██▉       | 44/153 [25:05<1:18:00, 42.94s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  29%|██▉       | 45/153 [25:54<1:20:46, 44.87s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  30%|███       | 46/153 [26:26<1:13:06, 40.99s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  31%|███       | 47/153 [27:19<1:18:31, 44.45s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  31%|███▏      | 48/153 [28:31<1:32:32, 52.88s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  32%|███▏      | 49/153 [29:10<1:24:18, 48.64s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  33%|███▎      | 50/153 [30:14<1:31:17, 53.18s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  33%|███▎      | 51/153 [30:45<1:19:00, 46.48s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  34%|███▍      | 52/153 [31:48<1:26:43, 51.52s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  35%|███▍      | 53/153 [32:43<1:27:27, 52.48s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  35%|███▌      | 54/153 [33:36<1:27:06, 52.80s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  36%|███▌      | 55/153 [34:41<1:32:10, 56.44s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  37%|███▋      | 56/153 [35:26<1:25:47, 53.07s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  37%|███▋      | 57/153 [35:52<1:11:37, 44.76s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  38%|███▊      | 58/153 [36:36<1:10:49, 44.74s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  39%|███▊      | 59/153 [37:30<1:14:26, 47.51s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  39%|███▉      | 60/153 [38:14<1:11:49, 46.34s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  40%|███▉      | 61/153 [38:56<1:09:06, 45.07s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  41%|████      | 62/153 [39:40<1:07:57, 44.81s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  41%|████      | 63/153 [40:55<1:20:54, 53.94s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  42%|████▏     | 64/153 [41:54<1:21:54, 55.21s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  42%|████▏     | 65/153 [42:37<1:15:55, 51.77s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  43%|████▎     | 66/153 [43:28<1:14:33, 51.42s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  44%|████▍     | 67/153 [44:18<1:13:16, 51.12s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  44%|████▍     | 68/153 [45:38<1:24:31, 59.67s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No attribute, try again.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Claims:  45%|████▌     | 69/153 [46:29<1:20:03, 57.18s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  46%|████▌     | 70/153 [47:38<1:23:59, 60.72s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  46%|████▋     | 71/153 [48:24<1:16:58, 56.33s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  47%|████▋     | 72/153 [49:28<1:18:58, 58.50s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  48%|████▊     | 73/153 [50:20<1:15:22, 56.53s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  48%|████▊     | 74/153 [51:19<1:15:27, 57.31s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  49%|████▉     | 75/153 [52:20<1:16:04, 58.52s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No attribute, try again.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Claims:  50%|████▉     | 76/153 [53:18<1:14:39, 58.18s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  50%|█████     | 77/153 [54:06<1:09:52, 55.17s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  51%|█████     | 78/153 [55:03<1:09:40, 55.74s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  52%|█████▏    | 79/153 [56:01<1:09:24, 56.28s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  52%|█████▏    | 80/153 [56:24<56:18, 46.28s/it]  Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  53%|█████▎    | 81/153 [56:52<49:09, 40.96s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  54%|█████▎    | 82/153 [57:55<56:09, 47.46s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  54%|█████▍    | 83/153 [58:43<55:42, 47.75s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  55%|█████▍    | 84/153 [59:42<58:50, 51.16s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  56%|█████▌    | 85/153 [1:00:24<54:43, 48.29s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  56%|█████▌    | 86/153 [1:00:42<43:55, 39.33s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  57%|█████▋    | 87/153 [1:01:32<46:49, 42.56s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  58%|█████▊    | 88/153 [1:02:27<49:57, 46.11s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  58%|█████▊    | 89/153 [1:03:05<46:48, 43.88s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  59%|█████▉    | 90/153 [1:03:50<46:14, 44.04s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  59%|█████▉    | 91/153 [1:04:27<43:12, 41.82s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  60%|██████    | 92/153 [1:05:09<42:36, 41.91s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  61%|██████    | 93/153 [1:06:08<47:02, 47.04s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  61%|██████▏   | 94/153 [1:06:53<45:40, 46.45s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  62%|██████▏   | 95/153 [1:07:59<50:42, 52.46s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  63%|██████▎   | 96/153 [1:08:38<45:50, 48.26s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  63%|██████▎   | 97/153 [1:09:24<44:31, 47.71s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  64%|██████▍   | 98/153 [1:10:01<40:44, 44.44s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  65%|██████▍   | 99/153 [1:10:49<41:05, 45.65s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  65%|██████▌   | 100/153 [1:11:24<37:25, 42.36s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  66%|██████▌   | 101/153 [1:12:22<40:42, 46.97s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  67%|██████▋   | 102/153 [1:13:17<42:09, 49.59s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  67%|██████▋   | 103/153 [1:14:49<51:42, 62.05s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  68%|██████▊   | 104/153 [1:15:19<43:01, 52.68s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  69%|██████▊   | 105/153 [1:16:05<40:31, 50.67s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  69%|██████▉   | 106/153 [1:17:14<43:58, 56.13s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  70%|██████▉   | 107/153 [1:17:51<38:29, 50.21s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  71%|███████   | 108/153 [1:18:43<38:04, 50.77s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  71%|███████   | 109/153 [1:20:03<43:42, 59.59s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No attribute, try again.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Claims:  72%|███████▏  | 110/153 [1:20:56<41:17, 57.62s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  73%|███████▎  | 111/153 [1:22:10<43:46, 62.54s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  73%|███████▎  | 112/153 [1:22:53<38:44, 56.70s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  74%|███████▍  | 113/153 [1:23:50<37:53, 56.83s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  75%|███████▍  | 114/153 [1:25:08<41:01, 63.11s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No attribute, try again.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Claims:  75%|███████▌  | 115/153 [1:25:51<36:07, 57.04s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  76%|███████▌  | 116/153 [1:26:55<36:24, 59.04s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  76%|███████▋  | 117/153 [1:27:57<35:58, 59.95s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  77%|███████▋  | 118/153 [1:28:24<29:20, 50.30s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  78%|███████▊  | 119/153 [1:29:36<32:12, 56.83s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  78%|███████▊  | 120/153 [1:30:17<28:33, 51.91s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No attribute, try again.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Claims:  79%|███████▉  | 121/153 [1:31:25<30:20, 56.89s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  80%|███████▉  | 122/153 [1:32:09<27:15, 52.77s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  80%|████████  | 123/153 [1:33:06<27:05, 54.18s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  81%|████████  | 124/153 [1:33:44<23:54, 49.46s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  82%|████████▏ | 125/153 [1:34:57<26:20, 56.44s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  82%|████████▏ | 126/153 [1:35:32<22:28, 49.96s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  83%|████████▎ | 127/153 [1:36:18<21:05, 48.68s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  84%|████████▎ | 128/153 [1:37:04<20:00, 48.02s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  84%|████████▍ | 129/153 [1:37:51<19:03, 47.65s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  85%|████████▍ | 130/153 [1:39:04<21:10, 55.24s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No attribute, try again.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Claims:  86%|████████▌ | 131/153 [1:40:29<23:28, 64.04s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  86%|████████▋ | 132/153 [1:41:53<24:32, 70.12s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  87%|████████▋ | 133/153 [1:42:43<21:24, 64.22s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  88%|████████▊ | 134/153 [1:43:24<18:05, 57.13s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  88%|████████▊ | 135/153 [1:44:02<15:25, 51.39s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  89%|████████▉ | 136/153 [1:44:57<14:54, 52.61s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  90%|████████▉ | 137/153 [1:45:44<13:35, 50.97s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  90%|█████████ | 138/153 [1:46:42<13:14, 52.95s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  91%|█████████ | 139/153 [1:47:15<10:55, 46.83s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  92%|█████████▏| 140/153 [1:48:08<10:33, 48.74s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  92%|█████████▏| 141/153 [1:49:13<10:45, 53.80s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  93%|█████████▎| 142/153 [1:50:08<09:55, 54.10s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  93%|█████████▎| 143/153 [1:51:05<09:09, 54.95s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  94%|█████████▍| 144/153 [1:52:18<09:02, 60.26s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  95%|█████████▍| 145/153 [1:52:58<07:13, 54.18s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  95%|█████████▌| 146/153 [1:54:25<07:29, 64.23s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  96%|█████████▌| 147/153 [1:55:47<06:56, 69.50s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No attribute, try again.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Claims:  97%|█████████▋| 148/153 [1:56:31<05:08, 61.72s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  97%|█████████▋| 149/153 [1:57:35<04:10, 62.53s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  98%|█████████▊| 150/153 [1:58:23<02:54, 58.22s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims:  99%|█████████▊| 151/153 [1:59:51<02:14, 67.12s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No attribute, try again.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Claims:  99%|█████████▉| 152/153 [2:00:31<00:58, 58.82s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Claims: 100%|██████████| 153/153 [2:01:26<00:00, 47.62s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All done.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzGuzHPE87Ya"
      },
      "source": [
        "# 3.Testing and Evaluation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Compute Recall and F-score to evaluate evidence retrieval output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZVeNYIH9IaL"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import argparse\n",
        "from typing import List, Tuple, Optional\n",
        "\n",
        "def compute_metrics(\n",
        "    gt: List[str],\n",
        "    retrieved: List[str]\n",
        ") -> Tuple[int, Optional[float], Optional[float]]:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        gt: ground-truth evidence ID\n",
        "        retrieved: top-100 evidence ID \n",
        "\n",
        "    Returns:\n",
        "        tp: hit\n",
        "        recall: tp / len(gt) \n",
        "        precision: tp / len(retrieved)\n",
        "    \"\"\"\n",
        "    set_gt = set(gt)\n",
        "    set_ret = set(retrieved)\n",
        "    tp = len(set_gt & set_ret)\n",
        "    recall = tp / len(gt) if gt else None\n",
        "    precision = tp / len(retrieved) if retrieved else None\n",
        "    return tp, recall, precision\n",
        "\n",
        "def main(train_claims_path: str, top100_path: str):\n",
        "    # read json\n",
        "    with open(train_claims_path, 'r', encoding='utf-8') as f:\n",
        "        train_claims = json.load(f)\n",
        "    with open(top100_path, 'r', encoding='utf-8') as f:\n",
        "        top100 = json.load(f)\n",
        "\n",
        "    recalls = []\n",
        "    precisions = []\n",
        "\n",
        "    # each claim\n",
        "    for claim_id, claim_info in train_claims.items():\n",
        "        gt_list = claim_info.get(\"evidences\", [])\n",
        "        retrieved_list = top100.get(claim_id, {})\n",
        "\n",
        "        tp, recall, precision = compute_metrics(gt_list, retrieved_list)\n",
        "        recalls.append(recall if recall is not None else 0.0)\n",
        "        precisions.append(precision if precision is not None else 0.0)\n",
        "\n",
        "    # avg\n",
        "    avg_recall = sum(recalls) / len(recalls) if recalls else 0.0\n",
        "    avg_precision = sum(precisions) / len(precisions) if precisions else 0.0\n",
        "    if (avg_precision + avg_recall) > 0:\n",
        "        avg_f1 = 2 * avg_precision * avg_recall / (avg_precision + avg_recall)\n",
        "    else:\n",
        "        avg_f1 = 0.0\n",
        "    print(\"\\n=== Overall ===\")\n",
        "    print(f\"Average Recall@k   : {avg_recall:.3f}\")\n",
        "    print(f\"Average Precision@k: {avg_precision:.3f}\")\n",
        "    print(f\"Average F1@k       : {avg_f1:.3f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description=\"topk Recall Precision F-score\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--train_claims\",\n",
        "        type=str,\n",
        "        default=\"./data/dev-claims.json\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--top100\",\n",
        "        type=str,\n",
        "        default=\"./data/dev-claims-top10-dense.json\",\n",
        "    )\n",
        "    args = parser.parse_args()\n",
        "    main(args.train_claims, args.top100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mefSOe8eTmGP"
      },
      "source": [
        "## Object Oriented Programming codes here\n",
        "\n",
        "*You can use multiple code snippets. Just add more if needed*"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "nlp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
