{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32yCsRUo8H33"
      },
      "source": [
        "# 2025 COMP90042 Project\n",
        "*Make sure you change the file name with your group id.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCybYoGz8YWQ"
      },
      "source": [
        "# Readme\n",
        "*If there is something to be noted for the marker, please mention here.*\n",
        "\n",
        "*If you are planning to implement a program with Object Oriented Programming style, please put those the bottom of this ipynb file*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6po98qVA8bJD"
      },
      "source": [
        "# 1.DataSet Processing\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re, json, ujson, numpy as np\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk import download\n",
        "from rank_bm25 import BM25Okapi\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "download(\"punkt\")\n",
        "download(\"stopwords\")\n",
        "\n",
        "STOP = set(stopwords.words(\"english\"))\n",
        "STEM = PorterStemmer().stem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvff21Hv8zjk"
      },
      "outputs": [],
      "source": [
        "# ---------- 1. 读取 evidence.json ----------\n",
        "ev_path = Path(\"./data/evidence.json\")\n",
        "with ev_path.open(\"r\", encoding=\"utf-8\") as f:\n",
        "    evid_dict = ujson.load(f)\n",
        "\n",
        "evid_ids   = list(evid_dict.keys())\n",
        "raw_texts  = [evid_dict[eid] for eid in evid_ids]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cv = CountVectorizer(\n",
        "        lowercase=True,\n",
        "        ngram_range=(1, 2),\n",
        "        token_pattern=r\"(?u)\\b[a-z]+\\b\",    # 只要字母组成的词\n",
        ")\n",
        "\n",
        "def nltk_stem_preprocessor(text: str) -> str:\n",
        "    \"\"\"\n",
        "    先用正则粗清洗，再词干化，再过滤停用词，最后以空格连接——\n",
        "    CountVectorizer 将把空格视为 token 分界。\n",
        "    \"\"\"\n",
        "    tokens = re.findall(r\"[A-Za-z]+\", text.lower())\n",
        "    tokens = [STEM(t) for t in tokens if t not in STOP]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# 使用自定义预处理器，scikit‑learn 会在内部调用它\n",
        "cv.set_params(preprocessor=nltk_stem_preprocessor, stop_words=None)\n",
        "\n",
        "# fit 只是为了构造 analyzer；不关心矩阵\n",
        "cv.fit(raw_texts)\n",
        "analyzer = cv.build_analyzer()\n",
        "\n",
        "# ---------- 3. 得到每篇文档的 token 列表 -------------------\n",
        "token_corpus = [analyzer(doc) for doc in tqdm(raw_texts, desc=\"Tokenize\")]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------- 3. 构建 BM25 索引 ----------\n",
        "bm25 = BM25Okapi(token_corpus, k1=1.5, b=0.75)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def retrieve_topk(claim_text: str, topk: int = 100):\n",
        "    query_tokens = analyzer(claim_text)\n",
        "    scores       = bm25.get_scores(query_tokens)\n",
        "    idx_sorted   = np.argsort(scores)[-topk:][::-1]\n",
        "    return [(evid_ids[i], float(scores[i])) for i in idx_sorted]\n",
        "\n",
        "# -------- DEMO --------\n",
        "demo_claim = \"South Australia has the most expensive electricity in the world.\"\n",
        "top_hits   = retrieve_topk(demo_claim, 5)   # [(id, score), ...]\n",
        "\n",
        "\n",
        "print(\"Top-5 results:\\n\")\n",
        "for rank, (eid, score) in enumerate(top_hits, 1):\n",
        "    print(f\"#{rank:02d}  {eid}   BM25={score:.4f}\")\n",
        "    print(\"     \", evid_dict[eid])\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# batch process\n",
        "def process_claim_file(claim_json: str, out_json: str):\n",
        "    with open(claim_json, \"r\", encoding=\"utf-8\") as f:\n",
        "        claims = json.load(f)            # {claim_id: {...}}\n",
        "    results = {}\n",
        "    for cid, obj in tqdm(claims.items(), desc=\"Retrieve\"):\n",
        "        hits = retrieve_topk(obj[\"claim_text\"])\n",
        "        results[cid] = {\"evidences\": [h[0] for h in hits]}\n",
        "    with open(out_json, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(results, f, ensure_ascii=False, indent=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# process_claim_file(\"./data/train-claims.json\", \"./data/train-claims-top100.json\")\n",
        "# process_claim_file(\"./data/dev-claims.json\", \"./data/dev-claims-top100.json\")\n",
        "# process_claim_file(\"./data/test-claims-unlabelled.json\", \"./data/test-claims-top100.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json, ujson, numpy as np\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "DATA_DIR   = Path(\"data\")\n",
        "TOP100_FNS = {\n",
        "    \"train\": \"train-claims-top100.json\",\n",
        "    \"dev\"  : \"dev-claims-top100.json\",\n",
        "    \"test\" : \"test-claims-top100.json\"\n",
        "}\n",
        "TOP_M = 6\n",
        "\n",
        "# ---------- 0. evidence ----------\n",
        "with (DATA_DIR / \"evidence.json\").open() as f:\n",
        "    evid_dict = ujson.load(f)\n",
        "evid_ids = list(evid_dict.keys())\n",
        "id2row   = {eid: i for i, eid in enumerate(evid_ids)}\n",
        "\n",
        "# ---------- 1. encode evidence ----------\n",
        "print(\"Encoding evidence vectors ...\")\n",
        "bi_model   = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "BATCH = 1024\n",
        "chunks = []\n",
        "for i in tqdm(range(0, len(evid_ids), BATCH)):\n",
        "    txts = [evid_dict[eid] for eid in evid_ids[i:i+BATCH]]\n",
        "    chunks.append(\n",
        "        bi_model.encode(txts, batch_size=32, normalize_embeddings=True).astype(\"float32\")\n",
        "    )\n",
        "evid_matrix = np.vstack(chunks)\n",
        "\n",
        "# ---------- 2. process each split ----------\n",
        "for split, fn in TOP100_FNS.items():\n",
        "    top100_path = DATA_DIR / fn\n",
        "    if not top100_path.exists():\n",
        "        continue\n",
        "\n",
        "    with top100_path.open() as f:\n",
        "        top100 = ujson.load(f)\n",
        "\n",
        "    claim_texts = {}\n",
        "    cfile = DATA_DIR / f\"{split}-claims.json\"\n",
        "\n",
        "    with cfile.open() as f:\n",
        "        raw = ujson.load(f)\n",
        "    claim_texts = {\n",
        "        cid: raw[cid][\"claim_text\"] if isinstance(raw[cid], dict) else raw[cid] for cid in raw\n",
        "    }\n",
        "\n",
        "    dense_out, text_out = {}, {}\n",
        "    for cid, entry in tqdm(top100.items(), desc=f\"{split} rerank\"):\n",
        "        id_list = entry[\"evidences\"] if isinstance(entry, dict) else entry\n",
        "        claim_emb = bi_model.encode(claim_texts.get(cid, \"\"), normalize_embeddings=True)\n",
        "        vecs = evid_matrix[[id2row[eid] for eid in id_list]]\n",
        "        scores = vecs @ claim_emb\n",
        "        top_idx = scores.argsort()[-TOP_M:][::-1]\n",
        "        top_ids = [id_list[i] for i in top_idx]\n",
        "\n",
        "        # dense\n",
        "        dense_out[cid] = top_ids\n",
        "\n",
        "        # text\n",
        "        text_out[cid] = {\n",
        "            \"claim_text\": claim_texts.get(cid, \"\"),\n",
        "            \"ranked_evidences\": [\n",
        "                {\"id\": eid, \"text\": evid_dict[eid]} for eid in top_ids\n",
        "            ]\n",
        "        }\n",
        "\n",
        "    # output\n",
        "    (DATA_DIR / f\"{split}-claims-top{TOP_M}-dense.json\").write_text(\n",
        "        json.dumps(dense_out, ensure_ascii=False, indent=2), encoding=\"utf-8\"\n",
        "    )\n",
        "    (DATA_DIR / f\"{split}-claims-top{TOP_M}-text.json\").write_text(\n",
        "        json.dumps(text_out, ensure_ascii=False, indent=2), encoding=\"utf-8\"\n",
        "    )\n",
        "\n",
        "print(\"All splits processed ✅\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FA2ao2l8hOg"
      },
      "source": [
        "# 2. Model Implementation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You are a fact-checking assistant. For the given Claim and Candidate Evidences, determine the correct Claim Label and list the IDs of those evidences you deem relevant.The label is [SUPPORTS, REFUTES, NOT_ENOUGH_INFO, DISPUTED]\n",
            "\"claim-2152\": \"Venus doesn't have a runaway greenhouse effect\",\n",
            "  \"evidences\": [\n",
            "    {\"evidence-1018575\": \"A runaway greenhouse effect involving carbon dioxide and water vapor has long ago been hypothesized to have occurred on Venus, this idea is still largely accepted.\"},\n",
            "  ],\n",
            "label: \"REFUTES\"\n",
            "Now, given the following claim and its candidate evidences, please output in the JSON format: {\"label\":\"\", \"evidences\":\"\"}\n",
            "\"claim-2967\": \"The contribution of waste heat to the global climate is 0.028 W/m2.\",\n",
            "  \"evidences\": [\n",
            "    {\"evidence-308923\": \"Global forcing from waste heat was 0.028 W/m2 in 2005.\"},\n",
            "    {\"evidence-1185839\": \"It could prove to be the most inexorable, however, if we are fortunate enough to evade all the rest.” Simple global-scale estimates that recently have been actualized and confirmed by more refined model calculations show noticeable contributions from waste heat to global warming after the year 2100, if its growth rates are not strongly reduced (below the averaged 2% p.a.\"},\n",
            "    {\"evidence-43606\": \"Waste heat generated by energy usage is a secondary contributor.\"},\n",
            "    {\"evidence-19067\": \"However, during last two decades there has been remarkable attention to recover waste heat from various industries and to optimize the units which are used to absorb heat from waste gases.\"},\n",
            "    {\"evidence-302863\": \"The resulting low-temperature waste heat is then used for water or space heating.\"},\n",
            "    {\"evidence-1096760\": \"This is more valuable and flexible than low-grade waste heat, but there is a slight loss of power generation.\"},\n",
            "  ]\n",
            "label:\n",
            "evidences:\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "def build_prompt_with_example(json_path: str) -> str:\n",
        "    # 1. few‐shot 示例\n",
        "    example = {\n",
        "        \"claim-2152\": {\n",
        "            \"claim_text\": \"Venus doesn't have a runaway greenhouse effect\",\n",
        "            \"ranked_evidences\": [\n",
        "                {\n",
        "                    \"id\": \"evidence-1018575\",\n",
        "                    \"text\": (\n",
        "                        \"A runaway greenhouse effect involving carbon dioxide and water vapor \"\n",
        "                        \"has long ago been hypothesized to have occurred on Venus, this idea \"\n",
        "                        \"is still largely accepted.\"\n",
        "                    )\n",
        "                }\n",
        "            ],\n",
        "            \"claim_label\": \"REFUTES\",\n",
        "            \"evidences\": [\"evidence-1018575\"]\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # 2. 读取 test‐claims-top6-text.json 并取第一条\n",
        "    with open(json_path, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "    test_claim_id, test_claim = next(iter(data.items()))\n",
        "    \n",
        "    # 3. 拼接 prompt\n",
        "    prompt_lines = []\n",
        "    prompt_lines.append(\"You are a fact-checking assistant. \"\n",
        "                 \"For the given Claim and Candidate Evidences, \"\n",
        "                 \"determine the correct Claim Label and list the IDs of those evidences you deem relevant.\"\n",
        "                 \"The label is [SUPPORTS, REFUTES, NOT_ENOUGH_INFO, DISPUTED]\" )\n",
        "    for cid, info in example.items():\n",
        "        prompt_lines.append(f\"\\\"{cid}\\\": \\\"{info['claim_text']}\\\",\")\n",
        "        prompt_lines.append(\"  \\\"evidences\\\": [\")\n",
        "        for ev in info['ranked_evidences']:\n",
        "            prompt_lines.append(f\"    {{\\\"{ev['id']}\\\": \\\"{ev['text']}\\\"}},\")\n",
        "        prompt_lines.append(\"  ],\")\n",
        "        prompt_lines.append(f\"label: \\\"{info['claim_label']}\\\"\")\n",
        "\n",
        "    prompt_lines.append(f\"Now, given the following claim and its candidate evidences, \"\n",
        "                 \"please output in the JSON format: {\\\"label\\\":\\\"\\\", \\\"evidences\\\":\\\"\\\"}\")\n",
        "    prompt_lines.append(f\"\\\"{test_claim_id}\\\": \\\"{test_claim['claim_text']}\\\",\")\n",
        "    prompt_lines.append(\"  \\\"evidences\\\": [\")\n",
        "    for ev in test_claim['ranked_evidences']:\n",
        "        prompt_lines.append(f\"    {{\\\"{ev['id']}\\\": \\\"{ev['text']}\\\"}},\")\n",
        "    prompt_lines.append(\"  ]\")\n",
        "    prompt_lines.append(\"label:\")\n",
        "    prompt_lines.append(\"evidences:\")\n",
        "    return \"\\n\".join(prompt_lines)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    prompt = build_prompt_with_example('./data/test-claims-top6-text.json')\n",
        "    print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt 字符数：1813\n"
          ]
        }
      ],
      "source": [
        "char_len = len(prompt)\n",
        "print(f\"Prompt 字符数：{char_len}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e3df450c454743a084250bd05dfea872",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# model_name = \"Qwen/Qwen3-0.6B\"\n",
        "# model_name = \"Qwen/Qwen3-4B\"\n",
        "# model_name = \"Qwen/Qwen3-4B-FP8\"\n",
        "\n",
        "model_name = \"Qwen/Qwen3-1.7B\"\n",
        "# load the tokenizer and the model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|im_start|>user\n",
            "You are a fact-checking assistant. For the given Claim and Candidate Evidences, determine the correct Claim Label and list the IDs of those evidences you deem relevant.The label is [SUPPORTS, REFUTES, NOT_ENOUGH_INFO, DISPUTED]\n",
            "\"claim-2152\": \"Venus doesn't have a runaway greenhouse effect\",\n",
            "  \"evidences\": [\n",
            "    {\"evidence-1018575\": \"A runaway greenhouse effect involving carbon dioxide and water vapor has long ago been hypothesized to have occurred on Venus, this idea is still largely accepted.\"},\n",
            "  ],\n",
            "label: \"REFUTES\"\n",
            "Now, given the following claim and its candidate evidences, please output in the JSON format: {\"label\":\"\", \"evidences\":\"\"}\n",
            "\"claim-2967\": \"The contribution of waste heat to the global climate is 0.028 W/m2.\",\n",
            "  \"evidences\": [\n",
            "    {\"evidence-308923\": \"Global forcing from waste heat was 0.028 W/m2 in 2005.\"},\n",
            "    {\"evidence-1185839\": \"It could prove to be the most inexorable, however, if we are fortunate enough to evade all the rest.” Simple global-scale estimates that recently have been actualized and confirmed by more refined model calculations show noticeable contributions from waste heat to global warming after the year 2100, if its growth rates are not strongly reduced (below the averaged 2% p.a.\"},\n",
            "    {\"evidence-43606\": \"Waste heat generated by energy usage is a secondary contributor.\"},\n",
            "    {\"evidence-19067\": \"However, during last two decades there has been remarkable attention to recover waste heat from various industries and to optimize the units which are used to absorb heat from waste gases.\"},\n",
            "    {\"evidence-302863\": \"The resulting low-temperature waste heat is then used for water or space heating.\"},\n",
            "    {\"evidence-1096760\": \"This is more valuable and flexible than low-grade waste heat, but there is a slight loss of power generation.\"},\n",
            "  ]\n",
            "label:\n",
            "evidences:<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n",
            "471\n"
          ]
        }
      ],
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "]\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True,\n",
        "    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n",
        ")\n",
        "print(text)\n",
        "tokens = tokenizer(text)\n",
        "num_tokens = len(tokens.input_ids)\n",
        "print(num_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "thinking content: <think>\n",
            "Okay, let's tackle this fact-checking task. The user provided a claim and a list of candidate evidences, and I need to determine the correct label: SUPPORTS, REFUTES, NOT_ENOUGH_INFO, or DISPUTED. \n",
            "\n",
            "First, the claim is: \"The contribution of waste heat to the global climate is 0.028 W/m2.\" The evidences include several entries. Let me go through each one.\n",
            "\n",
            "Evidence 308923 says that in 2005, the global forcing from waste heat was 0.028 W/m². That seems directly related to the claim. But I need to check if this is accurate. However, the claim is about the contribution, and the evidence states a specific value for 2005. But wait, the claim is about the contribution being 0.028 W/m². If the evidence says that in 2005 it was 0.028, but maybe that's just a specific instance. But the claim is about the contribution in general. However, the evidence might not be the best because it's a specific year. Also, the other evidence mentions that after 2000, there's a noticeable contribution from waste heat, but it's only if growth rates aren't reduced. So maybe the evidence is not fully supporting the claim.\n",
            "\n",
            "Evidence 1185839 states that recent estimates show noticeable contributions from waste heat after 2000, but only if growth rates aren't reduced. This suggests that the contribution might be significant but conditional on growth rates. However, the claim is stating the exact value of 0.028 W/m². The evidence here is more about the potential contribution rather than the exact value. \n",
            "\n",
            "Evidence 43606 says that waste heat is a secondary contributor. That might mean it's a smaller part of the overall climate forcing, but the claim is about the contribution being 0.028. If it's a secondary contributor, maybe the actual value is lower. But the evidence doesn't directly state that. \n",
            "\n",
            "Evidence 19067 mentions that there's been attention to recover waste heat, but that's about recovery efforts, not the contribution. \n",
            "\n",
            "Evidence 302863 says that low-temperature waste heat is used for heating, but that's about usage, not the contribution to climate. \n",
            "\n",
            "Evidence 1096760 talks about the value and flexibility of low-temperature waste heat but again, not the contribution to climate. \n",
            "\n",
            "So, the key evidence here is evidence 308923, which states 0.028 W/m² in 2005. However, the claim is about the contribution being 0.028 W/m². If the evidence is from 2005, and the claim is about the contribution being that value, then the evidence supports the claim. However, the other evidence (1185839) suggests that the contribution is noticeable after 2000, but only if growth rates aren't reduced. But the claim is stating a specific value. \n",
            "\n",
            "Wait, the claim is \"the contribution of waste heat to the global climate is 0.028 W/m2.\" If the evidence says that in 2005 it was 0.028, that would support the claim. But the other evidence says that after 2000, there's a noticeable contribution, but only if growth rates aren't reduced. So maybe the 0.028 is a specific value that's been measured, but the claim is about the contribution being that exact value. However, the evidence might not be sufficient to confirm that the contribution is exactly 0.028 W/m². \n",
            "\n",
            "But the user's answer is supposed to be based on the given evidences. The label here is to be determined. The initial example had a claim about Venus and the evidence about the runaway greenhouse effect, which was labeled as REFUTES. But in this case, the claim is about the contribution of waste heat. \n",
            "\n",
            "Looking at the evidences again, evidence 308923 directly states the value of 0.028 W/m² in 2005. That's a specific number. However, the other evidence (1185839) mentions that recent estimates show contributions after 2000, but only if growth rates aren't reduced. So if the claim is that the contribution is exactly 0.028, then evidence 308923 supports that. But the other evidence might be about the potential contribution, not the exact value. \n",
            "\n",
            "But the user's example had a claim that was refuted by evidence. In this case, the claim is about the contribution being 0.028. If the evidence says that in 2005 it was 0.028, then that supports the claim. However, if the claim is about the contribution being 0.028, and the evidence is a specific value from 2005, then it supports the claim. \n",
            "\n",
            "But maybe there's a problem here. The evidence 308923 is a specific instance, but the claim is about the contribution in general. However, if the evidence is from 2005 and the claim is about the contribution being 0.028, then that's a supporting evidence. \n",
            "\n",
            "Alternatively, maybe the claim is incorrect because the actual contribution is different. But the evidence provided doesn't contradict the claim. The evidence states that in 2005, it was 0.028, which is the value claimed. So the label would be SUPPORTS. \n",
            "\n",
            "Wait, but the user's example had a claim that was refuted. In this case, the claim is about the contribution being 0.028. The evidence 308923 says that in 2005, it was 0.028. So that supports the claim. The other evidence might be about the potential contribution, but the exact value is given in the evidence. Therefore, the label should be SUPPORTS. \n",
            "\n",
            "But wait, the user's example had a claim that was refuted. In this case, the claim is about the contribution being 0.028. If the evidence says that in 2005 it was 0.028, then that supports the claim. So the label would be SUPPORTS. \n",
            "\n",
            "But maybe the evidence is not sufficient. For example, if the evidence is from 2005, but the claim is about the contribution in general. However, the evidence is a specific value. So the label would be SUPPORTS. \n",
            "\n",
            "Alternatively, maybe the claim is that the contribution is 0.028, but the evidence is that in 2005 it was 0.028. So that's a supporting evidence. \n",
            "\n",
            "Therefore, the label should be SUPPORTS, and the evidences are evidence-308923. But wait, the user's example had a label of REFUTES. But in this case, the evidence supports the claim. So the label should be SUPPORTS. \n",
            "\n",
            "Wait, but the user's example had a claim that was refuted. Let me check again. The original example was: \"claim-2152\": \"Venus doesn't have a runaway greenhouse effect\", and the evidence was that the idea is still accepted. But the label was REFUTES. Wait, no, the user's example was:\n",
            "\n",
            "\"claim-2152\": \"Venus doesn't have a runaway greenhouse effect\",\n",
            "  \"evidences\": [\n",
            "    {\"evidence-1018575\": \"A runaway greenhouse effect involving carbon dioxide and water vapor has long ago been hypothesized to have occurred on Venus, this idea is still largely accepted.\"},\n",
            "  ],\n",
            "label: \"REFUTES\"\n",
            "\n",
            "But that's not correct. Because the claim is that Venus doesn't have a runaway greenhouse effect, but the evidence says that the idea is still accepted. So the label would be REFUTES because the evidence supports the claim that Venus does have it. Wait, no. If the claim is that Venus doesn't have it, and the evidence says that the idea is still accepted, then the evidence supports the claim that it does have it. Therefore, the label should be SUPPORTS. But the user's example says the label is REFUTES. That's a contradiction. Maybe the user made a mistake. But regardless, in this case, the user's example is a bit confusing. \n",
            "\n",
            "But back to the current problem. The claim is about the contribution of waste heat being 0.028 W/m². The evidence 308923 states that in 2005, the global forcing from waste heat was 0.028. So that's a specific value. Therefore, the label is SUPPORTS. The other evidences are not directly related. Evidence 1185839 talks about the potential contribution after 2000, but that's conditional. Evidence 43606 says it's a secondary contributor, which might mean it's not the main contributor, but the claim is about the contribution being 0.028. So the evidence 308923 directly supports the claim. Therefore, the label is SUPPORTS, and the evidence is evidence-308923. \n",
            "\n",
            "But wait, the user's example had a label of REFUTES. But in this case, the label should be SUPPORTS. So the final answer would be {\"label\":\"SUPPORTS\", \"evidences\":[\"evidence-308923\"]}.\n",
            "</think>\n",
            "content: {\"label\":\"SUPPORTS\", \"evidences\":[\"evidence-308923\"]}\n"
          ]
        }
      ],
      "source": [
        "# prepare the model input\n",
        "\n",
        "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# conduct text completion\n",
        "generated_ids = model.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=32768\n",
        ")\n",
        "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n",
        "\n",
        "# parsing thinking content\n",
        "try:\n",
        "    # rindex finding 151668 (</think>)\n",
        "    index = len(output_ids) - output_ids[::-1].index(151668)\n",
        "except ValueError:\n",
        "    index = 0\n",
        "\n",
        "thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
        "content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
        "\n",
        "print(\"thinking content:\", thinking_content)\n",
        "print(\"content:\", content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# ———— Config ————\n",
        "TEST_CLAIMS_FILE = './data/test-claims-top6-text.json'\n",
        "RESULTS_FILE     = 'results.json'\n",
        "CHECKPOINT_FILE  = 'checkpoint.json'\n",
        "MODEL_NAME       = 'Qwen/Qwen3-1.7B'\n",
        "\n",
        "# ———— Few-shot Example ————\n",
        "FEW_SHOT_EXAMPLE = {\n",
        "    \"claim-2152\": {\n",
        "        \"claim_text\": \"Venus doesn't have a runaway greenhouse effect\",\n",
        "        \"ranked_evidences\": [\n",
        "            {\n",
        "                \"id\": \"evidence-1018575\",\n",
        "                \"text\": (\n",
        "                    \"A runaway greenhouse effect involving carbon dioxide and water vapor \"\n",
        "                    \"has long ago been hypothesized to have occurred on Venus, this idea \"\n",
        "                    \"is still largely accepted.\"\n",
        "                )\n",
        "            },\n",
        "            {\n",
        "                \"id\": \"evidence-791159\",\n",
        "                \"text\": (\n",
        "                    \"Venus receives about twice the sunlight that Earth does, which is \"\n",
        "                    \"thought to have contributed to its runaway greenhouse effect.\"\n",
        "                )\n",
        "            },\n",
        "            {\n",
        "                \"id\": \"evidence-500249\",\n",
        "                \"text\": (\n",
        "                    \"In the extreme, the planet Venus is thought to have experienced a \"\n",
        "                    \"very large increase in greenhouse effect over its lifetime, so much \"\n",
        "                    \"so that its poles have warmed sufficiently to render its surface \"\n",
        "                    \"temperature effectively isothermal.\"\n",
        "                )\n",
        "            }\n",
        "        ],\n",
        "        \"claim_label\": \"REFUTES\",\n",
        "        \"evidences\": [\"evidence-1018575\", \"evidence-791159\"]\n",
        "    }\n",
        "}\n",
        "\n",
        "# ———— Helpers ————\n",
        "def load_json(path, default):\n",
        "    if os.path.isfile(path):\n",
        "        with open(path, 'r', encoding='utf-8') as f:\n",
        "            return json.load(f)\n",
        "    return default\n",
        "\n",
        "def save_json(obj, path):\n",
        "    with open(path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(obj, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "def build_prompt(claim_id, claim_obj):\n",
        "    lines = []\n",
        "    # system/user instruction\n",
        "    lines.append(\n",
        "        \"You are a fact-checking assistant. \"\n",
        "        \"For the given Claim and Candidate Evidences, determine the correct Claim Label \"\n",
        "        \"and list the IDs of those evidences you deem relevant, at least one evidence. \"\n",
        "        \"The label is one of [SUPPORTS, REFUTES, NOT_ENOUGH_INFO, DISPUTED].\"\n",
        "    )\n",
        "    lines.append(\"\")  # blank\n",
        "\n",
        "    # few-shot block\n",
        "    for ex_id, ex in FEW_SHOT_EXAMPLE.items():\n",
        "        lines.append(f'\"{ex_id}\": \"{ex[\"claim_text\"]}\",')\n",
        "        lines.append(\"  \\\"ranked_evidences\\\": [\")\n",
        "        for ev in ex[\"ranked_evidences\"]:\n",
        "            lines.append(f'    {{\"{ev[\"id\"]}\": \"{ev[\"text\"]}\"}},')\n",
        "        lines.append(\"  ],\")\n",
        "        lines.append(f'label: \"{ex[\"claim_label\"]}\"')\n",
        "        lines.append(f'evidences: {ex[\"evidences\"]}')\n",
        "        lines.append(\"\")  # separator\n",
        "\n",
        "    # target claim\n",
        "    lines.append(\n",
        "        \"Now, given the following claim and its candidate evidences, \"\n",
        "        \"please output **only** valid JSON in the format: {\\\"label\\\":\\\"\\\", \\\"evidences\\\":[]}\"\n",
        "    )\n",
        "    lines.append(f'\"{claim_id}\": \"{claim_obj[\"claim_text\"]}\",')\n",
        "    lines.append(\"  \\\"evidences\\\": [\")\n",
        "    for ev in claim_obj[\"ranked_evidences\"]:\n",
        "        lines.append(f'    {{\"{ev[\"id\"]}\": \"{ev[\"text\"]}\"}},')\n",
        "    lines.append(\"  ]\")\n",
        "    lines.append(\"label:\")\n",
        "    lines.append(\"evidences:\")\n",
        "    return \"\\n\".join(lines)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model Qwen/Qwen3-1.7B ...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "59712180e67e492dba8ac7149b3538df",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "Qwen3ForCausalLM(\n",
              "  (model): Qwen3Model(\n",
              "    (embed_tokens): Embedding(151936, 2048)\n",
              "    (layers): ModuleList(\n",
              "      (0-27): 28 x Qwen3DecoderLayer(\n",
              "        (self_attn): Qwen3Attention(\n",
              "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "          (k_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
              "          (v_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
              "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
              "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
              "        )\n",
              "        (mlp): Qwen3MLP(\n",
              "          (gate_proj): Linear(in_features=2048, out_features=6144, bias=False)\n",
              "          (up_proj): Linear(in_features=2048, out_features=6144, bias=False)\n",
              "          (down_proj): Linear(in_features=6144, out_features=2048, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
              "        (post_attention_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
              "      )\n",
              "    )\n",
              "    (norm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
              "    (rotary_emb): Qwen3RotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ———— Load Model ————\n",
        "print(f\"Loading model {MODEL_NAME} ...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Claims:   0%|          | 0/153 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Claims:  84%|████████▎ | 128/153 [20:34<04:01,  9.64s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 69\u001b[39m\n\u001b[32m     66\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ All done.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     25\u001b[39m model_inputs = tokenizer([text], return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m).to(model.device)\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# conduct text completion\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m generated_ids = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32768\u001b[39;49m\n\u001b[32m     31\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m output_ids = generated_ids[\u001b[32m0\u001b[39m][\u001b[38;5;28mlen\u001b[39m(model_inputs.input_ids[\u001b[32m0\u001b[39m]):].tolist()\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# parsing thinking content\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\77280\\anaconda3\\envs\\nlp\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\77280\\anaconda3\\envs\\nlp\\Lib\\site-packages\\transformers\\generation\\utils.py:2465\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[39m\n\u001b[32m   2457\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2458\u001b[39m         input_ids=input_ids,\n\u001b[32m   2459\u001b[39m         expand_size=generation_config.num_return_sequences,\n\u001b[32m   2460\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2461\u001b[39m         **model_kwargs,\n\u001b[32m   2462\u001b[39m     )\n\u001b[32m   2464\u001b[39m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2465\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2466\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2467\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2468\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2469\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2470\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2471\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2472\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2473\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2475\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2476\u001b[39m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[32m   2477\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2478\u001b[39m         input_ids=input_ids,\n\u001b[32m   2479\u001b[39m         expand_size=generation_config.num_beams,\n\u001b[32m   2480\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2481\u001b[39m         **model_kwargs,\n\u001b[32m   2482\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\77280\\anaconda3\\envs\\nlp\\Lib\\site-packages\\transformers\\generation\\utils.py:3434\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   3432\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   3433\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3434\u001b[39m     outputs = \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   3436\u001b[39m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[32m   3437\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n\u001b[32m   3438\u001b[39m     outputs,\n\u001b[32m   3439\u001b[39m     model_kwargs,\n\u001b[32m   3440\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   3441\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\77280\\anaconda3\\envs\\nlp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\77280\\anaconda3\\envs\\nlp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\77280\\anaconda3\\envs\\nlp\\Lib\\site-packages\\transformers\\utils\\generic.py:965\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    962\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    964\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m965\u001b[39m     output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    966\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    967\u001b[39m         output = output.to_tuple()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\77280\\anaconda3\\envs\\nlp\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\77280\\anaconda3\\envs\\nlp\\Lib\\site-packages\\transformers\\models\\qwen3\\modeling_qwen3.py:850\u001b[39m, in \u001b[36mQwen3ForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    845\u001b[39m output_hidden_states = (\n\u001b[32m    846\u001b[39m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.output_hidden_states\n\u001b[32m    847\u001b[39m )\n\u001b[32m    849\u001b[39m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m850\u001b[39m outputs: BaseModelOutputWithPast = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    851\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    852\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    853\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    854\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    855\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    856\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    857\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    858\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    859\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    860\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    861\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    863\u001b[39m hidden_states = outputs.last_hidden_state\n\u001b[32m    864\u001b[39m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\77280\\anaconda3\\envs\\nlp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\77280\\anaconda3\\envs\\nlp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\77280\\anaconda3\\envs\\nlp\\Lib\\site-packages\\transformers\\utils\\generic.py:965\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    962\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    964\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m965\u001b[39m     output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    966\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    967\u001b[39m         output = output.to_tuple()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\77280\\anaconda3\\envs\\nlp\\Lib\\site-packages\\transformers\\models\\qwen3\\modeling_qwen3.py:576\u001b[39m, in \u001b[36mQwen3Model.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[39m\n\u001b[32m    564\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    565\u001b[39m         partial(decoder_layer.\u001b[34m__call__\u001b[39m, **flash_attn_kwargs),\n\u001b[32m    566\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    573\u001b[39m         position_embeddings,\n\u001b[32m    574\u001b[39m     )\n\u001b[32m    575\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m576\u001b[39m     layer_outputs = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    577\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    578\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    579\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    580\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    581\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    583\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    584\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    585\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    586\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    588\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    590\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\77280\\anaconda3\\envs\\nlp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\77280\\anaconda3\\envs\\nlp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\77280\\anaconda3\\envs\\nlp\\Lib\\site-packages\\transformers\\models\\qwen3\\modeling_qwen3.py:289\u001b[39m, in \u001b[36mQwen3DecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    286\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.input_layernorm(hidden_states)\n\u001b[32m    288\u001b[39m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m hidden_states, self_attn_weights = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    293\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    300\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    302\u001b[39m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\77280\\anaconda3\\envs\\nlp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\77280\\anaconda3\\envs\\nlp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\77280\\anaconda3\\envs\\nlp\\Lib\\site-packages\\transformers\\models\\qwen3\\modeling_qwen3.py:218\u001b[39m, in \u001b[36mQwen3Attention.forward\u001b[39m\u001b[34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[39m\n\u001b[32m    215\u001b[39m hidden_shape = (*input_shape, -\u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.head_dim)\n\u001b[32m    217\u001b[39m query_states = \u001b[38;5;28mself\u001b[39m.q_norm(\u001b[38;5;28mself\u001b[39m.q_proj(hidden_states).view(hidden_shape)).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m key_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mk_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mk_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_shape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m.transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m    219\u001b[39m value_states = \u001b[38;5;28mself\u001b[39m.v_proj(hidden_states).view(hidden_shape).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m    221\u001b[39m cos, sin = position_embeddings\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\77280\\anaconda3\\envs\\nlp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\77280\\anaconda3\\envs\\nlp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# ———— Main Loop ————\n",
        "def main():\n",
        "    test_data   = load_json(TEST_CLAIMS_FILE, {})\n",
        "    results     = load_json(RESULTS_FILE, {})\n",
        "    checkpoint  = load_json(CHECKPOINT_FILE, {\"last_id\": None})\n",
        "    started = checkpoint[\"last_id\"] is None\n",
        "\n",
        "    for cid, claim in tqdm(test_data.items(), desc=\"Claims\"):\n",
        "        # skip until after last checkpoint\n",
        "        if not started:\n",
        "            if cid == checkpoint[\"last_id\"]:\n",
        "                started = True\n",
        "            continue\n",
        "\n",
        "        prompt = build_prompt(cid, claim)\n",
        "        # apply chat template\n",
        "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "        text = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True,\n",
        "            enable_thinking=True\n",
        "        )\n",
        "        # print(text)\n",
        "        model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "        # conduct text completion\n",
        "        generated_ids = model.generate(\n",
        "            **model_inputs,\n",
        "            max_new_tokens=32768\n",
        "        )\n",
        "        output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n",
        "\n",
        "        # parsing thinking content\n",
        "        try:\n",
        "        # rindex finding 151668 (</think>)\n",
        "            index = len(output_ids) - output_ids[::-1].index(151668)\n",
        "        except ValueError:\n",
        "            index = 0\n",
        "\n",
        "        content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
        "        \n",
        "        # parse JSON\n",
        "        try:\n",
        "            # clean\n",
        "            m = re.search(r'\\{.*\\}', content, flags=re.DOTALL)\n",
        "            json_str = m.group(0) if m else content\n",
        "\n",
        "            parsed = json.loads(json_str)\n",
        "            results[cid] = {\n",
        "                \"claim_label\": parsed[\"label\"],\n",
        "                \"evidences\":   parsed[\"evidences\"]\n",
        "            }\n",
        "        except json.JSONDecodeError:\n",
        "            print(f\"[WARN] JSON parse failed for {cid}, raw output:\\n{content}\")\n",
        "            break\n",
        "\n",
        "        # persist\n",
        "        save_json(results, RESULTS_FILE)\n",
        "        checkpoint[\"last_id\"] = cid\n",
        "        save_json(checkpoint, CHECKPOINT_FILE)\n",
        "\n",
        "        # courtesy pause\n",
        "        time.sleep(0.5)\n",
        "\n",
        "    print(\"✅ All done.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzGuzHPE87Ya"
      },
      "source": [
        "# 3.Testing and Evaluation\n",
        "(You can add as many code blocks and text blocks as you need. However, YOU SHOULD NOT MODIFY the section title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZVeNYIH9IaL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mefSOe8eTmGP"
      },
      "source": [
        "## Object Oriented Programming codes here\n",
        "\n",
        "*You can use multiple code snippets. Just add more if needed*"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "nlp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
