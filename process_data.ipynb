{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d628d6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, json, ujson, numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk import download\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "download(\"punkt\")\n",
    "download(\"stopwords\")\n",
    "\n",
    "STOP = set(stopwords.words(\"english\"))\n",
    "STEM = PorterStemmer().stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877dd749",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(STOP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c59d14",
   "metadata": {},
   "source": [
    "Read evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dce503",
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_path = Path(\"./data/evidence.json\")\n",
    "with ev_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    evid_dict = ujson.load(f)\n",
    "\n",
    "evid_ids   = list(evid_dict.keys())\n",
    "raw_texts  = [evid_dict[eid] for eid in evid_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c062079",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenize:  63%|██████▎   | 760758/1208827 [02:40<01:20, 5546.32it/s]"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    # Unicode normalize\n",
    "    text = unicodedata.normalize(\"NFC\", text)\n",
    "    # \\u00AD = soft hyphen, \\u2010–\\u2014 = range of dashes, '-' ASCII hyphen\n",
    "    text = re.sub(r\"[\\u00AD\\u2010-\\u2014\\-]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def nltk_stem_preprocessor(text: str) -> str:\n",
    "    text = clean_text(text)\n",
    "    tokens = re.findall(r\"[A-Za-z]+\", text.lower())\n",
    "    tokens = [STEM(t) for t in tokens if t not in STOP]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "cv = CountVectorizer(\n",
    "        ngram_range=(1, 2),\n",
    "        preprocessor=nltk_stem_preprocessor,\n",
    "        tokenizer=lambda text: text.split(),\n",
    "        token_pattern=None,             \n",
    "        stop_words=None,\n",
    "    )\n",
    "analyzer = cv.build_analyzer()\n",
    "\n",
    "# TOKEN\n",
    "token_corpus = [analyzer(doc) for doc in tqdm(raw_texts, desc=\"Tokenize\")]\n",
    "\n",
    "bm25 = BM25Okapi(token_corpus, k1=1.2, b=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd33aa22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(\n",
    "    preprocessor=nltk_stem_preprocessor,\n",
    "    tokenizer=lambda s: s.split(),\n",
    "    token_pattern=None,\n",
    "    ngram_range=(1, 2)\n",
    ")\n",
    "# 文档-特征 稀疏矩阵，形状 (num_docs, num_features)\n",
    "tfidf_corpus = vectorizer.fit_transform(raw_texts)\n",
    "\n",
    "# ———— 定义检索函数 ————\n",
    "def retrieve_topk_tfidf(claim_text: str, topk: int = 100):\n",
    "    \"\"\"\n",
    "    基于 TF–IDF 和余弦相似度，返回得分最高的前 topk 条证据。\n",
    "    输出格式：[(evid_id, score), ...]\n",
    "    \"\"\"\n",
    "    # 将查询转为同维度稀疏向量\n",
    "    tfidf_query = vectorizer.transform([claim_text])\n",
    "    # 计算余弦相似度：点积 / (||q|| * ||d_i||)\n",
    "    # 但由于 TfidfVectorizer 已经做了 L2 归一化，直接点积即可得到余弦相似度\n",
    "    scores = (tfidf_corpus @ tfidf_query.T).toarray().ravel()\n",
    "    \n",
    "    # 取 topk 索引\n",
    "    idx_sorted = np.argsort(scores)[-topk:][::-1]\n",
    "    return [(evid_ids[i], float(scores[i])) for i in idx_sorted]\n",
    "\n",
    "# ———— 批量处理声明文件 ————\n",
    "def process_claim_file_tfidf(claim_json: str, out_json: str):\n",
    "    with open(claim_json, \"r\", encoding=\"utf-8\") as f:\n",
    "        claims = json.load(f)  # {claim_id: {...}}\n",
    "    results = {}\n",
    "    for cid, obj in tqdm(claims.items(), desc=\"Retrieve\"):\n",
    "        hits = retrieve_topk_tfidf(obj[\"claim_text\"], topk=100)\n",
    "        results[cid] = {\"evidences\": [h[0] for h in hits]}\n",
    "    with open(out_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb1e9dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieve: 100%|██████████| 154/154 [00:50<00:00,  3.05it/s]\n"
     ]
    }
   ],
   "source": [
    "process_claim_file_tfidf(\"./data/dev-claims.json\", \"./data/dev-claims-top100-tf-idf.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8050ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_topk(claim_text: str, topk: int = 200):\n",
    "    query_tokens = analyzer(claim_text)\n",
    "    scores       = bm25.get_scores(query_tokens)\n",
    "    idx_sorted   = np.argsort(scores)[-topk:][::-1]\n",
    "    return [(evid_ids[i], float(scores[i])) for i in idx_sorted]\n",
    "\n",
    "# batch process\n",
    "def process_claim_file(claim_json: str, out_json: str):\n",
    "    with open(claim_json, \"r\", encoding=\"utf-8\") as f:\n",
    "        claims = json.load(f)\n",
    "    results = {}\n",
    "    for cid, obj in tqdm(claims.items(), desc=\"Retrieve\"):\n",
    "        hits = retrieve_topk(obj[\"claim_text\"])\n",
    "        results[cid] = {\"evidences\": [h[0] for h in hits]}\n",
    "    with open(out_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd73ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_claim_file(\"./data/dev-claims.json\", \"./data/dev-claims-top200-bigram-noise.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3333d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def truncate_top100(in_path: str, out_path: str, topk: int = 100):\n",
    "    # 1. 读入已有的 top200 结果\n",
    "    with open(in_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # 2. 对每个 claim，只保留前 topk 条 evidence id\n",
    "    truncated = {}\n",
    "    for cid, obj in data.items():\n",
    "        evids = obj.get(\"evidences\", [])\n",
    "        truncated[cid] = {\"evidences\": evids[:topk]}\n",
    "\n",
    "    # 3. 写出到新的 JSON 文件\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(truncated, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# 调用示例\n",
    "truncate_top100(\n",
    "    \"./data/dev-claims-top200-bigram-noise.json\",\n",
    "    \"./data/dev-claims-top100-bigram-noise.json\",\n",
    "    topk=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3e894c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, ujson, numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "DATA_DIR   = Path(\"data\")\n",
    "TOP100_FNS = {\n",
    "    \"train\": \"train-claims-top100.json\",\n",
    "    \"dev\"  : \"dev-claims-top100.json\",\n",
    "    \"test\" : \"test-claims-top100.json\"\n",
    "}\n",
    "TOP_M = 20\n",
    "\n",
    "# ---------- 0. evidence ----------\n",
    "with (DATA_DIR / \"evidence.json\").open() as f:\n",
    "    evid_dict = ujson.load(f)\n",
    "evid_ids = list(evid_dict.keys())\n",
    "id2row   = {eid: i for i, eid in enumerate(evid_ids)}\n",
    "\n",
    "# ---------- 1. encode evidence ----------\n",
    "print(\"Encoding evidence vectors ...\")\n",
    "bi_model   = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "BATCH = 1024\n",
    "chunks = []\n",
    "for i in tqdm(range(0, len(evid_ids), BATCH)):\n",
    "    txts = [evid_dict[eid] for eid in evid_ids[i:i+BATCH]]\n",
    "    chunks.append(\n",
    "        bi_model.encode(txts, batch_size=32, normalize_embeddings=True).astype(\"float32\")\n",
    "    )\n",
    "evid_matrix = np.vstack(chunks)\n",
    "\n",
    "# ---------- 2. process each split ----------\n",
    "for split, fn in TOP100_FNS.items():\n",
    "    top100_path = DATA_DIR / fn\n",
    "    if not top100_path.exists():\n",
    "        continue\n",
    "\n",
    "    with top100_path.open() as f:\n",
    "        top100 = ujson.load(f)\n",
    "\n",
    "    claim_texts = {}\n",
    "    cfile = DATA_DIR / f\"{split}-claims.json\"\n",
    "\n",
    "    with cfile.open() as f:\n",
    "        raw = ujson.load(f)\n",
    "    claim_texts = {\n",
    "        cid: raw[cid][\"claim_text\"] if isinstance(raw[cid], dict) else raw[cid] for cid in raw\n",
    "    }\n",
    "\n",
    "    dense_out, text_out = {}, {}\n",
    "    for cid, entry in tqdm(top100.items(), desc=f\"{split} rerank\"):\n",
    "        id_list = entry[\"evidences\"] if isinstance(entry, dict) else entry\n",
    "        claim_emb = bi_model.encode(claim_texts.get(cid, \"\"), normalize_embeddings=True)\n",
    "        vecs = evid_matrix[[id2row[eid] for eid in id_list]]\n",
    "        scores = vecs @ claim_emb\n",
    "        top_idx = scores.argsort()[-TOP_M:][::-1]\n",
    "        top_ids = [id_list[i] for i in top_idx]\n",
    "\n",
    "        # dense\n",
    "        dense_out[cid] = top_ids\n",
    "\n",
    "        # text\n",
    "        text_out[cid] = {\n",
    "            \"claim_text\": claim_texts.get(cid, \"\"),\n",
    "            \"ranked_evidences\": [\n",
    "                {\"id\": eid, \"text\": evid_dict[eid]} for eid in top_ids\n",
    "            ]\n",
    "        }\n",
    "\n",
    "    # output\n",
    "    (DATA_DIR / f\"{split}-claims-top{TOP_M}-dense.json\").write_text(\n",
    "        json.dumps(dense_out, ensure_ascii=False, indent=2), encoding=\"utf-8\"\n",
    "    )\n",
    "    (DATA_DIR / f\"{split}-claims-top{TOP_M}-text.json\").write_text(\n",
    "        json.dumps(text_out, ensure_ascii=False, indent=2), encoding=\"utf-8\"\n",
    "    )\n",
    "\n",
    "print(\"All splits processed ✅\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
