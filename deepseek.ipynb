{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "498514f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c82be8f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜>You are a fact-checking assistant. For the given Claim and Candidate Evidences, determine the correct Claim Label and list the IDs of those evidences you deem relevant.The label is [SUPPORTS, REFUTES, NOT_ENOUGH_INFO, DISPUTED]\n",
      "\"claim-2152\": \"Venus doesn't have a runaway greenhouse effect\",\n",
      "  \"evidences\": [\n",
      "    {\"evidence-1018575\": \"A runaway greenhouse effect involving carbon dioxide and water vapor has long ago been hypothesized to have occurred on Venus, this idea is still largely accepted.\"},\n",
      "  ],\n",
      "label: \"REFUTES\"\n",
      "Now, given the following claim and its candidate evidences, please output in the JSON format: {\"label\":\"\", \"evidences\":\"\"}\n",
      "\"claim-2967\": \"The contribution of waste heat to the global climate is 0.028 W/m2.\",\n",
      "  \"evidences\": [\n",
      "    {\"evidence-308923\": \"Global forcing from waste heat was 0.028 W/m2 in 2005.\"},\n",
      "    {\"evidence-1185839\": \"It could prove to be the most inexorable, however, if we are fortunate enough to evade all the rest.” Simple global-scale estimates that recently have been actualized and confirmed by more refined model calculations show noticeable contributions from waste heat to global warming after the year 2100, if its growth rates are not strongly reduced (below the averaged 2% p.a.\"},\n",
      "    {\"evidence-43606\": \"Waste heat generated by energy usage is a secondary contributor.\"},\n",
      "    {\"evidence-19067\": \"However, during last two decades there has been remarkable attention to recover waste heat from various industries and to optimize the units which are used to absorb heat from waste gases.\"},\n",
      "    {\"evidence-302863\": \"The resulting low-temperature waste heat is then used for water or space heating.\"},\n",
      "    {\"evidence-1096760\": \"This is more valuable and flexible than low-grade waste heat, but there is a slight loss of power generation.\"},\n",
      "  ]\n",
      "label:\n",
      "evidences: [“refutes”, “disputed”, “not enough info”]\n",
      "</think>\n",
      "\n",
      "```json\n",
      "{\"label\": \"REFUTES\", \"evidences\": [\"evidence-1185839\", \"evidence-43606\"]}\n",
      "```<｜end▁of▁sentence｜>\n"
     ]
    }
   ],
   "source": [
    "device = next(model.parameters()).device\n",
    "input_text = prompt\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=2048)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68226861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a fact-checking assistant. For the given Claim and Candidate Evidences, determine the correct Claim Label and list the IDs of those evidences you deem relevant.The label is [SUPPORTS, REFUTES, NOT_ENOUGH_INFO, DISPUTED]\n",
      "\"claim-2152\": \"Venus doesn't have a runaway greenhouse effect\",\n",
      "  \"evidences\": [\n",
      "    {\"evidence-1018575\": \"A runaway greenhouse effect involving carbon dioxide and water vapor has long ago been hypothesized to have occurred on Venus, this idea is still largely accepted.\"},\n",
      "  ],\n",
      "label: \"REFUTES\"\n",
      "Now, given the following claim and its candidate evidences, please output in the JSON format: {\"label\":\"\", \"evidences\":\"\"}\n",
      "\"claim-2967\": \"The contribution of waste heat to the global climate is 0.028 W/m2.\",\n",
      "  \"evidences\": [\n",
      "    {\"evidence-308923\": \"Global forcing from waste heat was 0.028 W/m2 in 2005.\"},\n",
      "    {\"evidence-1185839\": \"It could prove to be the most inexorable, however, if we are fortunate enough to evade all the rest.” Simple global-scale estimates that recently have been actualized and confirmed by more refined model calculations show noticeable contributions from waste heat to global warming after the year 2100, if its growth rates are not strongly reduced (below the averaged 2% p.a.\"},\n",
      "    {\"evidence-43606\": \"Waste heat generated by energy usage is a secondary contributor.\"},\n",
      "    {\"evidence-19067\": \"However, during last two decades there has been remarkable attention to recover waste heat from various industries and to optimize the units which are used to absorb heat from waste gases.\"},\n",
      "    {\"evidence-302863\": \"The resulting low-temperature waste heat is then used for water or space heating.\"},\n",
      "    {\"evidence-1096760\": \"This is more valuable and flexible than low-grade waste heat, but there is a slight loss of power generation.\"},\n",
      "  ]\n",
      "label:\n",
      "evidences:\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "def build_prompt_with_example(json_path: str) -> str:\n",
    "    # few‐shot\n",
    "    example = {\n",
    "        \"claim-2152\": {\n",
    "            \"claim_text\": \"Venus doesn't have a runaway greenhouse effect\",\n",
    "            \"ranked_evidences\": [\n",
    "                {\n",
    "                    \"id\": \"evidence-1018575\",\n",
    "                    \"text\": (\n",
    "                        \"A runaway greenhouse effect involving carbon dioxide and water vapor \"\n",
    "                        \"has long ago been hypothesized to have occurred on Venus, this idea \"\n",
    "                        \"is still largely accepted.\"\n",
    "                    )\n",
    "                }\n",
    "            ],\n",
    "            \"claim_label\": \"REFUTES\",\n",
    "            \"evidences\": [\"evidence-1018575\"]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    test_claim_id, test_claim = next(iter(data.items()))\n",
    "    \n",
    "    # prompt\n",
    "    prompt_lines = []\n",
    "    prompt_lines.append(\"You are a fact-checking assistant. \"\n",
    "                 \"For the given Claim and Candidate Evidences, \"\n",
    "                 \"determine the correct Claim Label and list the IDs of those evidences you deem relevant.\"\n",
    "                 \"The label is [SUPPORTS, REFUTES, NOT_ENOUGH_INFO, DISPUTED]\" )\n",
    "    for cid, info in example.items():\n",
    "        prompt_lines.append(f\"\\\"{cid}\\\": \\\"{info['claim_text']}\\\",\")\n",
    "        prompt_lines.append(\"  \\\"evidences\\\": [\")\n",
    "        for ev in info['ranked_evidences']:\n",
    "            prompt_lines.append(f\"    {{\\\"{ev['id']}\\\": \\\"{ev['text']}\\\"}},\")\n",
    "        prompt_lines.append(\"  ],\")\n",
    "        prompt_lines.append(f\"label: \\\"{info['claim_label']}\\\"\")\n",
    "\n",
    "    prompt_lines.append(f\"Now, given the following claim and its candidate evidences, \"\n",
    "                 \"please output in the JSON format: {\\\"label\\\":\\\"\\\", \\\"evidences\\\":\\\"\\\"}\")\n",
    "    prompt_lines.append(f\"\\\"{test_claim_id}\\\": \\\"{test_claim['claim_text']}\\\",\")\n",
    "    prompt_lines.append(\"  \\\"evidences\\\": [\")\n",
    "    for ev in test_claim['ranked_evidences']:\n",
    "        prompt_lines.append(f\"    {{\\\"{ev['id']}\\\": \\\"{ev['text']}\\\"}},\")\n",
    "    prompt_lines.append(\"  ]\")\n",
    "    prompt_lines.append(\"label:\")\n",
    "    prompt_lines.append(\"evidences:\")\n",
    "    return \"\\n\".join(prompt_lines)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    prompt = build_prompt_with_example('./data/test-claims-top6-text.json')\n",
    "    print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ef0fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n",
    ")\n",
    "print(text)\n",
    "tokens = tokenizer(text)\n",
    "num_tokens = len(tokens.input_ids)\n",
    "print(num_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
